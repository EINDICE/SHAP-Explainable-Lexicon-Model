{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1pmoyMAfY_JjxENYjg6oxM67o8e6CeMwu",
      "authorship_tag": "ABX9TyMkDQtm74ogVcYX9RsHeMMM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User input"
      ],
      "metadata": {
        "id": "ok9WwfwOZpMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter the location of the processed datasets\n",
        "drive_location = '/content/drive/MyDrive/nasdaq/processed/'"
      ],
      "metadata": {
        "id": "5s8sO8X1ZrWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ExplainableLexiconGenerator"
      ],
      "metadata": {
        "id": "eVWPXAJCBQ2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfAxXTD0GKAB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class ExplainableLexiconGenerator:\n",
        "\n",
        "  def __init__(self, positive_words_dataset, negative_words_dataset, positive_lm_dataset, negative_lm_dataset, word_column, aggregate_function = None):\n",
        "    # datasets\n",
        "    self.positive_words_dataset = positive_words_dataset\n",
        "    self.negative_words_dataset = negative_words_dataset\n",
        "    self.datasets = [self.positive_words_dataset, self.negative_words_dataset]\n",
        "    \n",
        "    self.positive_lm_dataset = positive_lm_dataset\n",
        "    self.negative_lm_dataset = negative_lm_dataset\n",
        "    self.lm_datasets = [self.positive_lm_dataset, self.negative_lm_dataset]\n",
        "    \n",
        "    # columns\n",
        "    self.word_column = word_column\n",
        "    self.sum_shap_values_column = 'sum_shap_values'\n",
        "    self.average_shap_values_column = 'average_shap_values'\n",
        "    self.count_column = 'count'\n",
        "    self.total_count_column = 'total'\n",
        "    self.word_count_column = 'word_count'\n",
        "    self.ratio_column = 'ratio'\n",
        "    self.category_column = 'category'\n",
        "    self.max_shap_values_column = 'max_shap_values'\n",
        "    self.min_shap_values_column = 'min_shap_values'\n",
        "    self.sentence_uuid_column = 'sentence_uuid'\n",
        "\n",
        "    # prefixes\n",
        "    self.opposite_prefix = 'opposite_'\n",
        "    self.tm_column_prefix = 'TM_'\n",
        "    self.lm_column_prefix = 'LM_'\n",
        "\n",
        "    self.columns_in_both_datasets = [self.count_column, self.sum_shap_values_column, self.average_shap_values_column, self.max_shap_values_column, self.min_shap_values_column,\n",
        "                                     self.ratio_column]\n",
        "\n",
        "    # values\n",
        "    self.positive_category_value = 'positive'\n",
        "    self.negative_category_value = 'negative'\n",
        "    self.tm_source_value = 'OUR_WORDS'\n",
        "    self.lm_source_value = 'LM'\n",
        "    self.aggregate_function = {self.count_column: 'sum', self.total_count_column: 'sum', \n",
        "                               self.sum_shap_values_column: 'sum', self.average_shap_values_column: 'mean', \n",
        "                               self.max_shap_values_column: 'max', self.min_shap_values_column: 'min', self.sentence_uuid_column: ','.join} \\\n",
        "                                if aggregate_function == None else aggregate_function\n",
        "\n",
        "    self.source_column = 'src'  \n",
        "\n",
        "\n",
        "  def __find_all_duplicates(self, dataset):\n",
        "    all_duplicates = dataset[dataset.duplicated(subset=[self.word_column]) == True]\n",
        "    unique_duplicated_words = all_duplicates[self.word_column].unique()\n",
        "\n",
        "    return unique_duplicated_words\n",
        "\n",
        "\n",
        "  def __remove_duplicates_in_dataset(self, dataset):\n",
        "    unique_duplicated_words = self.__find_all_duplicates(dataset)\n",
        "\n",
        "    # aggreagate words based on aggregate_function\n",
        "    dataset = dataset.groupby(by=[self.word_column]).agg(self.aggregate_function).reset_index()\n",
        "\n",
        "    # calculate the average_shap_values for dataset by dividing the sum_shap_values with the total count\n",
        "    duplicated_rows = dataset.loc[dataset[self.word_column].isin(unique_duplicated_words)]\n",
        "    averaged_values = duplicated_rows[self.sum_shap_values_column].values / duplicated_rows[self.total_count_column].values\n",
        "    dataset.loc[dataset[self.word_column].isin(unique_duplicated_words), self.average_shap_values_column] = averaged_values\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def check_for_duplicates(self, dataset):\n",
        "    all_duplicates = dataset[dataset.duplicated(subset=[self.word_column]) == True]\n",
        "    \n",
        "    assert len(all_duplicates) == 0, \"datasets contain duplicates\"\n",
        "\n",
        "  def remove_duplicates_within_datasets(self, datasets):\n",
        "    datasets = [self.__remove_duplicates_in_dataset(dataset.copy(True)) for dataset in datasets]\n",
        "\n",
        "    # check if there are no duplicates left in the datasets\n",
        "    [self.check_for_duplicates(dataset) for dataset in datasets]\n",
        "\n",
        "    return datasets\n",
        "  \n",
        "  def add_new_columns(self, dataset, category_value):\n",
        "    # adding 1 initially for ratio_column and word_count_column\n",
        "    dataset = self.__add_new_column_to_dataset(dataset, self.ratio_column, 1)\n",
        "    dataset = self.__add_new_column_to_dataset(dataset, self.word_count_column, 1)\n",
        "    dataset = self.__add_new_column_to_dataset(dataset, self.category_column, category_value)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __add_new_column_to_dataset(self, dataset, column, value):\n",
        "    dataset[column] = value\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def add_opposite_columns(self, dataset):\n",
        "    modified_datasets = [self.__add_new_column_to_dataset(dataset, f'{self.opposite_prefix}{opposite_column}', 0) for opposite_column in self.columns_in_both_datasets]\n",
        "\n",
        "    return modified_datasets[-1]\n",
        "\n",
        "  def rearrange_columns(self, dataset):\n",
        "    # rearranging columns in a specific way\n",
        "    initial_columns = [self.word_column, self.count_column, self.total_count_column, f'{self.opposite_prefix}{self.count_column}', self.category_column]\n",
        "\n",
        "    datasets_columns = self.columns_in_both_datasets[1:]\n",
        "    opposite_columns = [f'{self.opposite_prefix}{column}' for column in datasets_columns]\n",
        "\n",
        "    new_column_order = initial_columns + datasets_columns + opposite_columns + [self.word_count_column, self.sentence_uuid_column]\n",
        "\n",
        "    dataset = dataset.reindex(columns=new_column_order)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def add_all_necessary_columns(self, dataset, category_value):\n",
        "    dataset = self.add_new_columns(dataset, category_value)\n",
        "    dataset = self.add_opposite_columns(dataset)\n",
        "    dataset = self.rearrange_columns(dataset)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __find_overlapping_words_between_datasets(self, positive_dataset, negative_dataset):\n",
        "    # intersecting between the words from positive and negative dataset, thus finding the overlapping ones\n",
        "    positive_unique_words = set(positive_dataset[self.word_column].unique())\n",
        "    negative_unique_words = set(negative_dataset[self.word_column].unique())\n",
        "\n",
        "    overlapping_words = positive_unique_words.intersection(negative_unique_words)\n",
        "    \n",
        "    return overlapping_words\n",
        "\n",
        "  def __categorize_word(self, word, positive_dataset, negative_dataset):\n",
        "    # column names in a list\n",
        "    columns = list(positive_dataset.columns)\n",
        "    # values as an array for the word in the positive and negative dataset\n",
        "    positive_occurence = positive_dataset.loc[positive_dataset[self.word_column] == word].values[0]\n",
        "    negative_occurence = negative_dataset.loc[negative_dataset[self.word_column] == word].values[0]\n",
        "\n",
        "    count_column_index = columns.index(self.count_column)\n",
        "    avg_shap_values_column_index = columns.index(self.average_shap_values_column)\n",
        "\n",
        "    # categorization decision making values\n",
        "    positive_decision_value = positive_occurence[count_column_index] * positive_occurence[avg_shap_values_column_index]\n",
        "    negative_decision_value = negative_occurence[count_column_index] * negative_occurence[avg_shap_values_column_index]\n",
        "\n",
        "    # finding column indexes\n",
        "    total_column_index = columns.index(self.total_count_column)\n",
        "\n",
        "    columns_in_both_datasets = self.columns_in_both_datasets[:-1]\n",
        "    opposite_columns = [f'{self.opposite_prefix}{column}' for column in columns_in_both_datasets]\n",
        "\n",
        "    ratio_column_index = columns.index(self.ratio_column)\n",
        "    opposite_ratio_column_index = columns.index(f'{self.opposite_prefix}{self.ratio_column}')\n",
        "    \n",
        "    sentence_uuid_column_index = columns.index(self.sentence_uuid_column)\n",
        "\n",
        "    # determining the selected and opposite dataset\n",
        "    if positive_decision_value > negative_decision_value:\n",
        "      selected_occurence = positive_occurence\n",
        "      opposite_occurence = negative_occurence\n",
        "    else:\n",
        "      selected_occurence = negative_occurence\n",
        "      opposite_occurence = positive_occurence\n",
        "\n",
        "    # adding all opposite values except ratio\n",
        "    for value, opposite_value in zip(columns_in_both_datasets, opposite_columns):\n",
        "      column_index = columns.index(opposite_value)\n",
        "      opposite_column_index = columns.index(value)\n",
        "      selected_occurence[column_index] = opposite_occurence[opposite_column_index]\n",
        "\n",
        "    # updating total count\n",
        "    selected_occurence[total_column_index] = selected_occurence[count_column_index] + opposite_occurence[count_column_index]\n",
        "\n",
        "    # adding ratio and opposite ratio\n",
        "    ratio = selected_occurence[avg_shap_values_column_index] / (selected_occurence[avg_shap_values_column_index] + opposite_occurence[avg_shap_values_column_index])\n",
        "    selected_occurence[ratio_column_index] = ratio\n",
        "    selected_occurence[opposite_ratio_column_index] = 1 - ratio\n",
        "\n",
        "    # updating sentence uuids\n",
        "    selected_occurence[sentence_uuid_column_index] = f'{selected_occurence[sentence_uuid_column_index]},{opposite_occurence[sentence_uuid_column_index]}'\n",
        "\n",
        "    return selected_occurence\n",
        "\n",
        "  def __add_word_occurences_to_datasets(self, categorized_words):\n",
        "    columns = self.positive_words_dataset.columns\n",
        "    category_column_index = columns.index(self.category_column)\n",
        "\n",
        "    positive_word_occurences, negative_word_occurences = [], []\n",
        "\n",
        "    # dividng the positive and negative word occurences\n",
        "    for word_occurence in categorized_words:\n",
        "      category_value = word_occurence[category_column_index]\n",
        "\n",
        "      positive_word_occurences.append(word_occurence) if self.positive_category_value == category_value else negative_word_occurences.append(word_occurence)\n",
        "\n",
        "    # appending to the existing datasets\n",
        "    new_positive_words_df = pd.DataFrame([positive_word_occurences], columns = columns)\n",
        "    self.positive_words_dataset = self.positive_words_dataset.append(new_positive_words_df, ignore_index = True)\n",
        "    \n",
        "    new_negative_words_df = pd.DataFrame([negative_word_occurences], columns = columns)\n",
        "    self.negative_words_dataset = self.negative_words_dataset.append(new_negative_words_df, ignore_index = True)\n",
        "\n",
        "  def remove_duplicates_between_datasets_and_merge_them(self, positive_dataset, negative_dataset):\n",
        "    overlapping_words = self.__find_overlapping_words_between_datasets(positive_dataset, negative_dataset)\n",
        "\n",
        "    # categorizing overlapping word occurences\n",
        "    categorized_word_occurences = []\n",
        "    [categorized_word_occurences.append(self.__categorize_word(word, positive_dataset, negative_dataset)) for word in overlapping_words]\n",
        "\n",
        "    # removing the occurences that contains the overlapping words from both datasets\n",
        "    positive_dataset = positive_dataset.loc[~positive_dataset[self.word_column].isin(overlapping_words)]\n",
        "    negative_dataset = negative_dataset.loc[~negative_dataset[self.word_column].isin(overlapping_words)]\n",
        "\n",
        "    categorized_words_df = pd.DataFrame(categorized_word_occurences, columns = positive_dataset.columns)\n",
        "  \n",
        "    combined_words_dataset = positive_dataset.append(negative_dataset, ignore_index = True)\n",
        "    combined_words_dataset = combined_words_dataset.append(categorized_words_df, ignore_index = True)\n",
        "\n",
        "    return combined_words_dataset\n",
        "\n",
        "  def remove_duplicates_from_lm_datasets_and_add_columns(self):\n",
        "    # just dropping duplicated words since we don't need any aggregation\n",
        "    lm_datasets = [dataset.copy(True) for dataset in self.lm_datasets]\n",
        "    [dataset.drop_duplicates(subset=[self.word_column], inplace=True) for dataset in lm_datasets]\n",
        "\n",
        "    column_to_add = [self.sum_shap_values_column, self.total_count_column, self.word_count_column,\n",
        "                     self.max_shap_values_column, self.min_shap_values_column,\n",
        "                     self.ratio_column, self.average_shap_values_column, self.count_column]\n",
        "\n",
        "    # adding all columns regarding each dataset with value 1\n",
        "    [[self.__add_new_column_to_dataset(dataset, column, 1) for column in column_to_add] for dataset in lm_datasets]\n",
        "\n",
        "    # adding opposite columns with value 0\n",
        "    [self.add_opposite_columns(dataset) for dataset in lm_datasets]\n",
        "\n",
        "    # adding special columns with string value and adding category value\n",
        "    [self.__add_new_column_to_dataset(dataset, self.sentence_uuid_column, '') for dataset in lm_datasets]\n",
        "\n",
        "\n",
        "    # adding category column to LM datasets\n",
        "    lm_positive_dataset, lm_negative_dataset = lm_datasets\n",
        "    lm_positive_dataset = self.__add_new_column_to_dataset(lm_positive_dataset, self.category_column, self.positive_category_value)\n",
        "    lm_negative_dataset = self.__add_new_column_to_dataset(lm_negative_dataset, self.category_column, self.negative_category_value)\n",
        "\n",
        "    # rearranging columns\n",
        "    lm_positive_dataset = self.rearrange_columns(lm_positive_dataset)\n",
        "    lm_negative_dataset = self.rearrange_columns(lm_negative_dataset)\n",
        "\n",
        "    return lm_positive_dataset, lm_negative_dataset\n",
        "\n",
        "  def __add_columns_prefix(self, dataset, prefix):\n",
        "    dataset.columns = [dataset.columns[0]] + [f'{prefix}{column}' for column in list(dataset.columns)[1:]]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def prepare_for_merging(self):\n",
        "    # removing duplicates from positive and negative datasets\n",
        "    positive_dataset, negative_dataset = self.remove_duplicates_within_datasets(self.datasets)\n",
        "    \n",
        "    # adding the required columns for the next calculations\n",
        "    positive_dataset = self.add_all_necessary_columns(positive_dataset, self.positive_category_value)\n",
        "    negative_dataset = self.add_all_necessary_columns(negative_dataset, self.negative_category_value)\n",
        "    \n",
        "    # removing duplicates between positive and negative datasets\n",
        "    combined_words_dataset =  self.remove_duplicates_between_datasets_and_merge_them(positive_dataset, negative_dataset)\n",
        "\n",
        "    # # processing LM words datasets\n",
        "    lm_positive_dataset, lm_negative_dataset = self.remove_duplicates_from_lm_datasets_and_add_columns()\n",
        "    lm_combined_words_dataset = self.remove_duplicates_between_datasets_and_merge_them(lm_positive_dataset, lm_negative_dataset)\n",
        "\n",
        "    return combined_words_dataset, lm_combined_words_dataset\n",
        "\n",
        "  def merge_our_and_lm_datasets(self, combined_words_dataset, lm_combined_words_dataset):\n",
        "    # adding TM_ and LM_ prefixes to the columns\n",
        "    combined_words_dataset = self.__add_columns_prefix(combined_words_dataset, self.tm_column_prefix)\n",
        "    lm_combined_words_dataset = self.__add_columns_prefix(lm_combined_words_dataset, self.lm_column_prefix)\n",
        "\n",
        "    # adding \"OUR WORDS\" and \"LM\" as source values in column to corresponding datasets\n",
        "    combined_words_dataset = self.__add_new_column_to_dataset(combined_words_dataset, f'{self.tm_column_prefix}{self.source_column}', self.tm_source_value)\n",
        "    lm_combined_words_dataset = self.__add_new_column_to_dataset(lm_combined_words_dataset, f'{self.lm_column_prefix}{self.source_column}', self.lm_source_value)\n",
        "\n",
        "    # merging our combined dataset with LM dataset with outer join\n",
        "    merged_dataset = pd.merge(combined_words_dataset, lm_combined_words_dataset, how=\"outer\", on=[self.word_column])\n",
        "\n",
        "    # we fill TM_category and LM_category columns with \"none\" values since that are words that do not appear for that dataset\n",
        "    merged_dataset[f'{self.tm_column_prefix}{self.category_column}'].fillna('none', inplace = True)\n",
        "    merged_dataset[f'{self.lm_column_prefix}{self.category_column}'].fillna('none', inplace = True)\n",
        "\n",
        "    # we fill TM_src that is na with \"LM\" as source value since that word is from LM words\n",
        "    # we fill LM_src that is na with \"OUR WORDS\" as source value since that word is from our words\n",
        "    # the words that appear in the both datasets will have \"OUR WORDS\" in the TM_src column and \"LM\" in the LM_src column\n",
        "    merged_dataset[f'{self.tm_column_prefix}{self.source_column}'].fillna(self.lm_source_value, inplace = True)\n",
        "    merged_dataset[f'{self.lm_column_prefix}{self.source_column}'].fillna(self.tm_source_value, inplace = True)\n",
        "\n",
        "    merged_dataset[f'{self.tm_column_prefix}{self.sentence_uuid_column}'].fillna('', inplace = True)\n",
        "    merged_dataset[f'{self.lm_column_prefix}{self.sentence_uuid_column}'].fillna('', inplace = True)\n",
        "\n",
        "    # we fill all other columns with 0 value because they don't have value for the other source and this way they will not interfere with the calculation\n",
        "    merged_dataset.fillna(0, inplace = True)\n",
        "\n",
        "    return merged_dataset\n",
        "\n",
        "  def generate_merged_dataset(self):\n",
        "    combined_words_dataset, lm_combined_words_dataset = self.prepare_for_merging()\n",
        "    merged_dataset = self.merge_our_and_lm_datasets(combined_words_dataset, lm_combined_words_dataset)\n",
        "    \n",
        "    return merged_dataset\n",
        "\n",
        "\n",
        "  def __normalize_column(self, dataset, column):\n",
        "    column_max_value = dataset[column].max()\n",
        "    if column_max_value == 0:\n",
        "      return dataset\n",
        "    \n",
        "    dataset[column] = dataset[column].apply(lambda value: value / column_max_value)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def normalize_dataset(self, dataset):\n",
        "    columns_to_normalize = dataset.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    print(f'Columns to normalize: {columns_to_normalize}')\n",
        "    \n",
        "    dataset_copy = dataset.copy(True)\n",
        "    modified_datasets = [self.__normalize_column(dataset_copy, column) for column in columns_to_normalize]\n",
        "\n",
        "    print('Columns normalized')\n",
        "\n",
        "    return modified_datasets[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def extract_dataset(files, dataset_type):\n",
        "  dataset_file = [f for f in files if dataset_type in f]\n",
        "\n",
        "  if not dataset_file:\n",
        "    raise Exception(f'{dataset_type} words dataset {dataset_file} was not found')\n",
        "\n",
        "  df = pd.read_csv(dataset_file[0])\n",
        "\n",
        "  return df\n",
        "\n",
        "def extract_pos_neg_datasets(files):\n",
        "  pos_df = extract_dataset(files, 'positive')\n",
        "  neg_df = extract_dataset(files, 'negative')\n",
        "\n",
        "  return pos_df, neg_df\n",
        "\n",
        "def extract_processed_datasets(location):\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from: {location} ...')\n",
        "\n",
        "  assert files_locations != 0, 'No files found in the provided location'\n",
        "\n",
        "  lm_words_files = [f for f in files_locations if 'lm' in f]\n",
        "  our_words_files = list(set(files_locations) - set(lm_words_files))\n",
        "\n",
        "  print(f'Reading TM datasets {our_words_files} ...')\n",
        "  our_dfs = extract_pos_neg_datasets(our_words_files)\n",
        "  \n",
        "  print(f'Reading LM datasets {lm_words_files} ...')\n",
        "  lm_dfs = extract_pos_neg_datasets(lm_words_files)\n",
        "  \n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "\n",
        "  return our_dfs + lm_dfs\n",
        "\n",
        "\n",
        "def save_lexicons(location, lexicons):\n",
        "  location_mod = location if location[-1] == '/' else f'{location}/'\n",
        "  parent_location = os.path.abspath(os.path.join(location_mod, os.pardir))\n",
        "  \n",
        "  lexicons_location = f'{parent_location}/lexicons'\n",
        "\n",
        "  if not os.path.exists(lexicons_location):\n",
        "    os.makedirs(lexicons_location)\n",
        "\n",
        "  for lexicon_name in lexicons:\n",
        "    loc = f'{lexicons_location}/{lexicon_name}'\n",
        "    print(f'Saving {lexicon_name} on location {loc}')\n",
        "    lexicons[lexicon_name].to_csv(loc, index = False)\n",
        "  \n",
        "  print(f'Lexicons saved on location: {lexicons_location}')"
      ],
      "metadata": {
        "id": "5lmuExr-OLMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP-LM lexicon generation"
      ],
      "metadata": {
        "id": "5PLe2LfbXDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# reading the sentiment datasets\n",
        "positive_words, negative_words, lm_pos_words, lm_neg_words = extract_processed_datasets(drive_location)\n",
        "\n",
        "# SHAP-LM lexicon generation\n",
        "explainableLexiconGenerator = ExplainableLexiconGenerator(positive_words, negative_words, lm_pos_words, lm_neg_words, 'word')\n",
        "merged_dataset = explainableLexiconGenerator.generate_merged_dataset()\n",
        "\n",
        "# creating normalized version of the SHAP-LM lexicon\n",
        "normalized_merged_df = explainableLexiconGenerator.normalize_dataset(merged_dataset)\n",
        "\n",
        "# saving lexicons to lexicon folder on drive\n",
        "lexicons = {'shap_lm_lexicon.csv': merged_dataset, 'normalized-shap_lm_lexicon.csv': normalized_merged_df}\n",
        "save_lexicons(drive_location, lexicons)"
      ],
      "metadata": {
        "id": "oaqkJcDT4yik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}