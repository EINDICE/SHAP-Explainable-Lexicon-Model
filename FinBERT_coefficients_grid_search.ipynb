{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMjZzmB6PTm"
      },
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twAm5QQn6PJG"
      },
      "outputs": [],
      "source": [
        "# enter the location of lexicons (please make sure that there are only lexicons files in the folder)\n",
        "lexicons_folder_map = {\n",
        "    'nasdaq': '/content/drive/MyDrive/finbert process/nasdaq/lexicons',\n",
        "    'fpb': '/content/drive/MyDrive/finbert process/fpb/lexicons',\n",
        "    'sentfin': '/content/drive/MyDrive/finbert process/sentfin/lexicons'\n",
        "}\n",
        "\n",
        "# enter the location of all evaluation datasets (please make sure that there are only evaluation files in the folder)\n",
        "eval_datasets_folder_loc = '/content/drive/MyDrive/datasets/evaluation datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8tFPSCnYtEv"
      },
      "source": [
        "# ShapDictModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOLcHcCZL6F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqVEyjNMQvwG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import nltk\n",
        "import torch\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ShapDictModel:\n",
        "  def __init__(self, dataset, tokenizer, word_column, category_column, decision_makers, count_column, dataset_source = None):\n",
        "    self.dataset_source = str.upper(dataset_source) if dataset_source is not None else 'both'\n",
        "    self.word_column = word_column\n",
        "    self.tokenizer = tokenizer\n",
        "    self.count_column = count_column\n",
        "\n",
        "    # Lemmatizer\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # source column values\n",
        "    self.lm_source = 'LM'\n",
        "    self.tm_source = 'OUR_WORDS'\n",
        "\n",
        "    # column prefix\n",
        "    self.lm_prefix = 'LM_'\n",
        "    self.tm_prefix = 'TM_'\n",
        "\n",
        "    # category values\n",
        "    self.positive_category_value = 'positive'\n",
        "    self.negative_category_value = 'negative'\n",
        "\n",
        "    # opposite prefix\n",
        "    self.opposite_prefix = 'opposite_'\n",
        "\n",
        "    # number of required coefficient when both sources or one sorce\n",
        "    self.coefficient_number_both_sources = 4\n",
        "    self.coefficient_number_one_source = 2\n",
        "\n",
        "    # prefix when one source is chosen\n",
        "    self.prefix = 'TM_' if self.dataset_source == self.tm_source else 'LM_' if self.dataset_source == self.lm_source else ''\n",
        "\n",
        "    # source column postfix\n",
        "    self.source_column = 'src'\n",
        "\n",
        "    # dataset on which results are calculated\n",
        "    self.dataset = dataset if self.dataset_source == 'both' else self.extract_dataset_from_source(dataset, self.dataset_source)\n",
        "\n",
        "    # function that will calculate the score\n",
        "    self.calculate_score = self.calculate_score_both_dataset_sources\n",
        "    # self.calculate_score = self.calculate_score_both_dataset_sources if self.dataset_source == 'both' else self.calculate_score_one_dataset_source\n",
        "\n",
        "    # category when one source is chosen\n",
        "    self.category = category_column\n",
        "\n",
        "    # decision makers\n",
        "    self.decision_makers = decision_makers\n",
        "    print()\n",
        "    print(f'Created ShapDictModel with decision makers: {self.decision_makers}')\n",
        "    print()\n",
        "\n",
        "\n",
        "  def extract_dataset_from_source(self, dataset, source, only_source_columns=True):\n",
        "    # depending on which source is chosen, the full dataset will be modified to return the required dataset\n",
        "\n",
        "    # if LM is chosen as source, then the returned dataset will contain only the words that were originally from the LM dataset with LM_ added as prefix to the columns\n",
        "    if source == self.lm_source:\n",
        "      prefix = self.lm_prefix\n",
        "      column = f'{self.lm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.tm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.lm_source\n",
        "      opposite_dataset_source = self.tm_source\n",
        "    else:\n",
        "      # if OUR_WORDS is chosen as source, then the returned dataset will contain only the words that were originally from our words dataset with TM_ added as prefix to the columns\n",
        "      prefix = self.tm_prefix\n",
        "      column = f'{self.tm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.lm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.tm_source\n",
        "      opposite_dataset_source = self.lm_source\n",
        "\n",
        "    source_dataset = dataset[(dataset[column] == dataset_source) & ((dataset[opposite_column] == dataset_source) | (dataset[opposite_column] == opposite_dataset_source))]\n",
        "\n",
        "    # filtering so just the necessary columns will remain\n",
        "    if only_source_columns:\n",
        "      columns = list(source_dataset.columns)\n",
        "      source_columns = [self.word_column] + [column for column in columns if prefix in column]\n",
        "\n",
        "      return source_dataset[source_columns]\n",
        "\n",
        "    return source_dataset\n",
        "\n",
        "  def calculate_score_both_dataset_sources(self, word_occurence):\n",
        "    tm_accumulated_score, tm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.tm_prefix)\n",
        "    lm_accumulated_score, lm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.lm_prefix)\n",
        "\n",
        "    return tm_accumulated_score, tm_opposite_accumulated_score, lm_accumulated_score, lm_opposite_accumulated_score\n",
        "\n",
        "  def predict_sentence_label(self, sentence, label_t):\n",
        "    words = self.tokenizer.tokenize(sentence)\n",
        "    cleaned_words = self.clean_and_lemmatize_words(words)\n",
        "\n",
        "    tm, tmo, lm, lmo = 0, 0, 0, 0\n",
        "    for word in cleaned_words:\n",
        "      # it can only have one occurence, but to check if it occurs at all\n",
        "      word_occurences = self.dataset.loc[self.dataset[self.word_column] == word].values\n",
        "      if len(word_occurences) == 0:\n",
        "        continue\n",
        "\n",
        "      word_occurence = word_occurences[0]\n",
        "\n",
        "      decision_scores = self.calculate_score(word_occurence)\n",
        "      tm += decision_scores[0]\n",
        "      tmo += decision_scores[1]\n",
        "      lm += decision_scores[2]\n",
        "      lmo += decision_scores[3]\n",
        "\n",
        "    return tm, tmo, lm, lmo\n",
        "\n",
        "  def __get_wordnet_pos(self, word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  def clean_and_lemmatize_words(self, words):\n",
        "    lower_case_words = [str(word).lower().replace('Ä¡', '').strip() for word in words]\n",
        "    return [self.lemmatizer.lemmatize(word, self.__get_wordnet_pos(word)) for word in lower_case_words]\n",
        "\n",
        "  def calculate_word_dm_score(self, word_occurence, decision_maker_column, count_column):\n",
        "    columns = list(self.dataset.columns)\n",
        "\n",
        "    decision_maker_index = columns.index(decision_maker_column)\n",
        "\n",
        "    value = word_occurence[decision_maker_index]\n",
        "\n",
        "    return value\n",
        "\n",
        "  def calculate_dataset_source_score(self, word_occurence, column_prefix):\n",
        "    columns = list(self.dataset.columns)\n",
        "    category_index = columns.index(f'{column_prefix}{self.category}')\n",
        "\n",
        "    word_category = word_occurence[category_index]\n",
        "\n",
        "    selected_category_sign = 1\n",
        "    opposite_category_sign = 1\n",
        "    if word_category == self.positive_category_value:\n",
        "      opposite_category_sign = -1\n",
        "    elif word_category == self.negative_category_value:\n",
        "      selected_category_sign = -1\n",
        "\n",
        "    if selected_category_sign == opposite_category_sign:\n",
        "      return 0, 0\n",
        "\n",
        "    opposite_column_prefix = f'{column_prefix}{self.opposite_prefix}'\n",
        "\n",
        "    selected_category_score = 0\n",
        "    opposite_category_score = 0\n",
        "    for decision_maker in self.decision_makers:\n",
        "      selected_category_score += self.calculate_word_dm_score(word_occurence, f'{column_prefix}{decision_maker}', f'{column_prefix}{self.count_column}') * selected_category_sign\n",
        "      opposite_category_score += self.calculate_word_dm_score(word_occurence, f'{opposite_column_prefix}{decision_maker}', f'{opposite_column_prefix}{self.count_column}') * opposite_category_sign\n",
        "\n",
        "    return selected_category_score, opposite_category_score\n",
        "\n",
        "  def calculate_model_accuracy(self, true_labels, predicted_labels):\n",
        "    accuracy_indicators = [true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)]\n",
        "\n",
        "    return np.asarray(accuracy_indicators).sum() / len(true_labels)\n",
        "\n",
        "\n",
        "  def predict_and_evaluate(self, sentences, true_labels):\n",
        "\n",
        "    calculated_scores = [self.predict_sentence_label(sentence, label) for sentence, label in zip(sentences, true_labels)]\n",
        "\n",
        "    return calculated_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS3kUmiCdvym"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pytz\n",
        "\n",
        "def create_summary_results(lexicon_source, lexicon_datasets, evaluation_datasets, drive_loc):\n",
        "\n",
        "  summary_df_items = []\n",
        "  for lexicon_name in lexicon_datasets:\n",
        "    lexicon = lexicon_datasets[lexicon_name]\n",
        "    lexicon_normalized = 'normalized' in lexicon_name\n",
        "\n",
        "    for evaluate_dataset_name in evaluation_datasets:\n",
        "      evaluate_dataset = evaluation_datasets[evaluate_dataset_name]\n",
        "      sentences = evaluate_dataset['text'].values\n",
        "      true_labels = evaluate_dataset['sentiment'].values\n",
        "\n",
        "      dataset = lexicon.copy(True)\n",
        "\n",
        "      lexicon_type = 'normalized' if lexicon_normalized else 'merged'\n",
        "\n",
        "      evaluation_result = evaluate(dataset, sentences, true_labels, lexicon_source, lexicon_normalized, evaluate_dataset_name)\n",
        "      summary_df_items = summary_df_items + evaluation_result\n",
        "\n",
        "  summary_df = pd.DataFrame(summary_df_items, columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset',\n",
        "                                                         'Sentence', 'True Label', 'XLex AS', 'XLex OAS', 'LM AS', 'LM OAS'])\n",
        "\n",
        "  summary_df.to_csv(f'{drive_loc}/summary_df.csv', index=False)\n",
        "\n",
        "  return summary_df\n",
        "\n",
        "def run_shap_dict_model(dataset, sentences, true_labels, dataset_source = None):\n",
        "  shap_dict_model = ShapDictModel(dataset, tokenizer, 'word', 'category', ['average_shap_values'], 'count', dataset_source = dataset_source)\n",
        "  result = shap_dict_model.predict_and_evaluate(sentences, true_labels)\n",
        "\n",
        "  return result\n",
        "\n",
        "def evaluate(dataset, sentences, true_labels, lexicon_source, lexicon_name, evaluate_dataset_name):\n",
        "  results = run_shap_dict_model(dataset, sentences, true_labels)\n",
        "\n",
        "  res_df_list = []\n",
        "  for sentence, label, res in zip(sentences, true_labels, results):\n",
        "    new_entry = [lexicon_source, lexicon_name, evaluate_dataset_name, sentence, label] + list(res)\n",
        "    res_df_list.append(new_entry)\n",
        "\n",
        "  return res_df_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fYDNTncaX_r"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "def calc_metrics(true_labels, predicted_labels):\n",
        "  results = []\n",
        "\n",
        "  acc = accuracy_score(true_labels, predicted_labels)\n",
        "  print(f'Accuracy score: {acc}')\n",
        "  results.append(acc)\n",
        "\n",
        "  pr = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  print(f'Precision score: {pr}')\n",
        "  results.append(pr)\n",
        "\n",
        "  rec = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  print(f'Recall score: {rec}')\n",
        "  results.append(rec)\n",
        "\n",
        "  f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  print(f'F1 score: {f1}')\n",
        "  results.append(f1)\n",
        "\n",
        "  mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
        "  print(f'MCC score: {mcc}')\n",
        "  results.append(mcc)\n",
        "\n",
        "  print()\n",
        "  print(\"Classification Report:\")\n",
        "  cl_report = classification_report(true_labels, predicted_labels, zero_division=0)\n",
        "  print(cl_report)\n",
        "\n",
        "  print()\n",
        "  print(\"Confusion Matrix:\")\n",
        "  conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "  print(conf_matrix)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def calc_label(decision_values):\n",
        "  return [1 if dv > 0 else 0 if dv < 0 else -1 for dv in decision_values]\n",
        "\n",
        "def calc_segment(coefs, df):\n",
        "  c1, c2, c3, c4 = coefs\n",
        "\n",
        "  xlex_decision_score = c1 * df['XLex AS'] + c2 * df['XLex OAS']\n",
        "  lm_decision_score =  c3 * df['LM AS'] + c4 * df['LM OAS']\n",
        "\n",
        "  return calc_label(lm_decision_score), calc_label(xlex_decision_score), calc_label(xlex_decision_score + lm_decision_score)\n",
        "\n",
        "\n",
        "def calc_row(coefs, dff, version, eval_df, extension = ''):\n",
        "  types = ['LMD', 'OUR', 'OUR + LMD']\n",
        "  true_labels = dff['True Label']\n",
        "  lex_name = dff['Lexicon Source'].values[0]\n",
        "  coef_df_values = []\n",
        "\n",
        "  whole_dataset = calc_segment(coefs, dff)\n",
        "  for wd, t in zip(whole_dataset, types):\n",
        "    metrics = calc_metrics(true_labels, wd)\n",
        "    new_row = [lex_name, version, eval_df, f'{t}{extension}', 'average_shap_values'] + coefs + metrics\n",
        "    coef_df_values.append(new_row)\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def calc_version(df, coefs, eval_dfs, version):\n",
        "  coef_df_values = []\n",
        "\n",
        "  for eval_df in eval_dfs:\n",
        "    dff = df[(df['Lexicon Normalized'] == version) & (df['Evaluation Dataset'] == eval_df)].copy(True).reset_index(drop=True)\n",
        "    coef_df_values = coef_df_values + calc_row(coefs, dff, version, eval_df)\n",
        "\n",
        "    dff_on_lm = dff[(dff['LM AS'] != 0) | (dff['LM OAS'] != 0)]\n",
        "    coef_df_values = coef_df_values + calc_row(coefs, dff_on_lm, version, eval_df, extension = ' on LMD')\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def calc_df(coefs, df):\n",
        "  versions = df['Lexicon Normalized'].unique()\n",
        "  eval_dfs = df['Evaluation Dataset'].unique()\n",
        "\n",
        "  coef_df_values = []\n",
        "\n",
        "  for version in versions:\n",
        "    coef_df_values = coef_df_values + calc_version(df, coefs, eval_dfs, version)\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def get_coefs_res(df, coefs):\n",
        "  columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Words Source',\n",
        "             'Decision Maker', 'C1', 'C2', 'C3', 'C4', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
        "\n",
        "  dff = df.copy(True)\n",
        "\n",
        "  coefs_map = {}\n",
        "\n",
        "  for coef in coefs:\n",
        "    coef_df_values = calc_df(coef, dff)\n",
        "    coefs_map[str(coef)] = pd.DataFrame(coef_df_values, columns = columns)\n",
        "\n",
        "  return coefs_map\n",
        "\n",
        "\n",
        "def get_metric_values(df, eval_df, normalized, metric):\n",
        "  word_sources = ['LMD', 'OUR', 'OUR + LMD', 'LMD on LMD', 'OUR on LMD', 'OUR + LMD on LMD']\n",
        "  all_metric_values = []\n",
        "\n",
        "  for ws in word_sources:\n",
        "    eval_df_mask = df['Evaluation Dataset'] == eval_df\n",
        "    lexicon_normalized_mask = df['Lexicon Normalized'] == normalized\n",
        "    word_source_mask = df['Words Source'] == ws\n",
        "\n",
        "    combined_mask = eval_df_mask & lexicon_normalized_mask & word_source_mask\n",
        "\n",
        "    metric_value = df[combined_mask][metric].values[0]\n",
        "\n",
        "    all_metric_values.append(metric_value)\n",
        "\n",
        "  return all_metric_values\n",
        "\n",
        "def create_summary_dataset(df, metric):\n",
        "  source_df = df['Lexicon Source'].unique()[0]\n",
        "  eval_dfs = df['Evaluation Dataset'].unique()\n",
        "  normalized = True\n",
        "  coefs = list(df.loc[0, ['C1', 'C2', 'C3', 'C4']])\n",
        "  decision_maker = 'average_shap_values'\n",
        "\n",
        "  summary_df_values = []\n",
        "\n",
        "  for ed in eval_dfs:\n",
        "\n",
        "    for n in [normalized, not normalized]:\n",
        "      metric_values = get_metric_values(df, ed, n, metric)\n",
        "      row_value = [source_df, n, ed, decision_maker] + coefs + metric_values\n",
        "      summary_df_values.append(row_value)\n",
        "\n",
        "  cols = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Decision Maker', 'C1', 'C2', 'C3', 'C4',\n",
        "          'LM', 'XLex', 'XLex + LM', 'LM on LM', 'XLex on LM', 'XLex + LM on LM']\n",
        "\n",
        "  return pd.DataFrame(summary_df_values, columns = cols)\n",
        "\n",
        "\n",
        "def coefficient_permutations(coefs, coefs_number):\n",
        "  return [p for p in itertools.product(coefs, repeat=coefs_number)]\n",
        "\n",
        "def calc_metric_acc_ratio(dff):\n",
        "  cols = ['XLex', 'XLex + LM', 'XLex on LM', 'XLex + LM on LM']\n",
        "  cols_lm = ['LM', 'LM', 'LM on LM', 'LM on LM']\n",
        "\n",
        "  res = []\n",
        "  sum_res = []\n",
        "\n",
        "  for c, c_lm in zip(cols, cols_lm):\n",
        "    res.append((dff[c] >= dff[c_lm]).sum())\n",
        "\n",
        "\n",
        "    sum_res.append(dff[c].sum())\n",
        "    sum_res.append((dff[c] - dff[c_lm]).sum())\n",
        "\n",
        "    sum_res.append(dff[c].mean())\n",
        "    sum_res.append((dff[c] - dff[c_lm]).mean())\n",
        "\n",
        "  return res, sum_res\n",
        "\n",
        "def make_str(nums, total):\n",
        "  res = []\n",
        "\n",
        "  for num in nums:\n",
        "    res.append(f'{str(num)}/{str(total)}')\n",
        "\n",
        "  return res\n",
        "\n",
        "def calc_perc(nums, total):\n",
        "  res = []\n",
        "\n",
        "  for num in nums:\n",
        "    perc = (num / total) * 100\n",
        "    res.append(perc)\n",
        "\n",
        "  return res\n",
        "\n",
        "def calc_metrics_ratio(dff):\n",
        "  metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
        "  rows = []\n",
        "\n",
        "  for metric in metrics:\n",
        "    df_metrics = pd.DataFrame()\n",
        "\n",
        "    summary_dataset = create_summary_dataset(dff, metric)\n",
        "    df_metrics = pd.concat([df_metrics, summary_dataset], ignore_index = True)\n",
        "\n",
        "    acc_ratios, sum_acc_ratios = calc_metric_acc_ratio(df_metrics)\n",
        "    total = len(df_metrics)\n",
        "\n",
        "    init_cols = df_metrics.loc[0, ['Lexicon Source', 'C1', 'C2', 'C3', 'C4']]\n",
        "\n",
        "    new_row = [init_cols[0]] + [metric] + list(init_cols[1:]) + make_str(acc_ratios, total) + calc_perc(acc_ratios, total) + sum_acc_ratios\n",
        "    rows.append(new_row)\n",
        "\n",
        "  return rows\n",
        "\n",
        "def calc_accuracy_ratio(df, coeffs):\n",
        "  cols = ['Lexicon Source', 'Metric', 'C1', 'C2', 'C3', 'C4', 'XLex', 'XLex + LM', 'XLex on LM', 'XLex + LM on LM',\n",
        "          '% XLex', '% XLex + LM', '% XLex on LM', '% XLex + LM on LM',\n",
        "          'XLex Sum Abs Value', 'XLex Sum Diff Value', 'XLex Avg Abs Value', 'XLex Avg Diff Value',\n",
        "          'XLex + LM Sum Abs Value', 'XLex + LM Sum Diff Value', 'XLex + LM Avg Abs Value', 'XLex + LM Avg Diff Value',\n",
        "          'XLex on LM Sum Abs Value', 'XLex on LM Sum Diff Value', 'XLex on LM Avg Abs Value', 'XLex on LM Avg Diff Value',\n",
        "          'XLex + LM on LM Sum Abs Value', 'XLex + LM on LM Sum Diff Value', 'XLex + LM on LM Avg Abs Value', 'XLex + LM on LM Avg Diff Value']\n",
        "  coefs_map = get_coefs_res(df, coeffs)\n",
        "  all_coef_dfs = list(coefs_map.values())\n",
        "\n",
        "  rows = []\n",
        "\n",
        "  for coef_df in all_coef_dfs:\n",
        "    new_rows = calc_metrics_ratio(coef_df)\n",
        "\n",
        "    rows = rows + new_rows\n",
        "\n",
        "  return pd.DataFrame(rows, columns = cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ZfUoZdNGPV"
      },
      "source": [
        "# Create Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnm_ohd6wAy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def extract_file_name(file_loc):\n",
        "  return file_loc.split('/')[-1].split('.')[0]\n",
        "\n",
        "def extract_datasets_map(datasets_location):\n",
        "  location = datasets_location if datasets_location[-1] == '/' else f'{datasets_location}/'\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from: {location} ...')\n",
        "\n",
        "  assert files_locations != 0, 'No files found in the provided location'\n",
        "\n",
        "  datasets_map = {}\n",
        "  for f in files_locations:\n",
        "    print(f'Reading dataset: {f} ...')\n",
        "    dataset = pd.read_csv(f)\n",
        "    datasets_map[extract_file_name(f)] = dataset\n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "\n",
        "  return datasets_map\n",
        "\n",
        "\n",
        "def create_results_folder(loc):\n",
        "  parent_location = os.path.abspath(os.path.join(loc, os.pardir))\n",
        "  mod_location = parent_location if parent_location[-1] == '/' else f'{parent_location}/'\n",
        "\n",
        "  results_location = f'{mod_location}raw results'\n",
        "\n",
        "  if not os.path.exists(results_location):\n",
        "    os.makedirs(results_location)\n",
        "\n",
        "  print(f'Created results dataset on location: {results_location} ...')\n",
        "\n",
        "  return results_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OThrhyy6Ct00"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "for lexicon_name in lexicons_folder_map:\n",
        "  lexicon_folder_loc = lexicons_folder_map[lexicon_name]\n",
        "\n",
        "  lexicon_datasets_map = extract_datasets_map(lexicon_folder_loc)\n",
        "\n",
        "  eval_datasets_map = extract_datasets_map(eval_datasets_folder_loc)\n",
        "\n",
        "  results_folder_loc = create_results_folder(lexicon_folder_loc)\n",
        "\n",
        "  # creating raw results which will later be used to find the most suitable coefficients\n",
        "  df = create_summary_results(lexicon_name, lexicon_datasets_map, eval_datasets_map, results_folder_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metric grid search from raw results"
      ],
      "metadata": {
        "id": "pbP0Tnqb-bTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexicons_raw_results_map = {\n",
        "    'nasdaq': '/content/drive/MyDrive/finbert process/nasdaq/raw results',\n",
        "    'fpb': '/content/drive/MyDrive/finbert process/fpb/raw results',\n",
        "    'sentfin': '/content/drive/MyDrive/finbert process/sentfin/raw results'\n",
        "}\n",
        "\n",
        "\n",
        "coefs = coefficient_permutations([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 3)\n",
        "full_coefs = []\n",
        "for c in coefs:\n",
        "  full_coefs.append(list(c) + [0.5])\n",
        "\n",
        "for lexicon_source in lexicons_raw_results_map:\n",
        "  lexicon_raw_results_loc = lexicons_raw_results_map[lexicon_source]\n",
        "\n",
        "  dff = pd.read_csv(lexicon_raw_results_loc)\n",
        "\n",
        "  full_dff = calc_accuracy_ratio(dff, full_coefs)\n",
        "\n",
        "  full_dff.to_csv(f'/content/drive/MyDrive/finbert process/{lexicon_source}/metrics_grid_search.csv', index=False)"
      ],
      "metadata": {
        "id": "D7z9KbqY_C11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract only valid combinations"
      ],
      "metadata": {
        "id": "fhB-Jc_6A0AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "nasdaq_dff = pd.read_csv('/content/drive/MyDrive/finbert process/nasdaq/metrics_grid_search.csv')\n",
        "fpb_dff = pd.read_csv('/content/drive/MyDrive/finbert process/fpb/metrics_grid_search.csv')\n",
        "sentfin_dff = pd.read_csv('/content/drive/MyDrive/finbert process/sentfin/metrics_grid_search.csv')"
      ],
      "metadata": {
        "id": "aleBE69UAwBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_valid(dff, columns):\n",
        "  for c in columns:\n",
        "    if dff[c].values[0] != 100:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def check_metrics(dff, metrics, columns):\n",
        "  valid = True\n",
        "  for m in metrics:\n",
        "    tmp = dff[dff['Metric'] == m]\n",
        "    val = check_if_valid(tmp, columns)\n",
        "    valid = valid and val\n",
        "\n",
        "  return valid\n",
        "\n",
        "def organize_coefs(df):\n",
        "  coefs = coefficient_permutations([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 3)\n",
        "  init_metrics = ['Accuracy', 'F1', 'MCC']\n",
        "  additional_metrics = ['Precision', 'Recall']\n",
        "\n",
        "  init_columns = ['% XLex + LM', '% XLex + LM on LM']\n",
        "  add_columns = ['% XLex']\n",
        "  extra_columns = ['% XLex on LM']\n",
        "\n",
        "  full_coefs = []\n",
        "  for c in coefs:\n",
        "    full_coefs.append(list(c) + [0.5])\n",
        "\n",
        "  ll = []\n",
        "\n",
        "  for fc in full_coefs:\n",
        "    l = []\n",
        "    l = l + fc\n",
        "    c1, c2, c3, c4 = fc\n",
        "\n",
        "    init_valid = True\n",
        "    tmp = df[(df['C1'] == c1) & (df['C2'] == c2) & (df['C3'] == c3) & (df['C4'] == c4)]\n",
        "\n",
        "    v1 = check_metrics(tmp, init_metrics, init_columns)\n",
        "    v2 = check_metrics(tmp, init_metrics, add_columns)\n",
        "    v3 = check_metrics(tmp, init_metrics, extra_columns)\n",
        "\n",
        "    l = l + [v1, v2, v3]\n",
        "\n",
        "    v1 = check_metrics(tmp, additional_metrics, init_columns)\n",
        "    v2 = check_metrics(tmp, additional_metrics, add_columns)\n",
        "    v3 = check_metrics(tmp, additional_metrics, extra_columns)\n",
        "\n",
        "    l = l + [v1, v2, v3]\n",
        "    ll.append(l)\n",
        "\n",
        "\n",
        "  return pd.DataFrame(ll, columns = ['C1', 'C2', 'C3', 'C4', 'Init M - XLex + LM', 'Init M - XLex', 'Init M - XLex on LM',\n",
        "                                     'Add M - XLex + LM', 'Add M - XLex', 'Add M - XLex on LM', ])\n",
        "\n",
        "\n",
        "def extract_successful_combinations(dfs):\n",
        "  c = 0\n",
        "  coefs_map = dict()\n",
        "  for df in dfs:\n",
        "    org_df = organize_coefs(df)\n",
        "    valid_combinations = org_df[(org_df['Init M - XLex + LM'] == True) & (org_df['Init M - XLex'] == True)].copy(True)\n",
        "    valid_combinations['Coefs'] = valid_combinations.apply(lambda row: ','.join([str(row['C1']),\n",
        "                                                                                  str(row['C2']),\n",
        "                                                                                  str(row['C3']),\n",
        "                                                                                  str(row['C4'])]), axis=1)\n",
        "    coefs_map[str(c)] = set(valid_combinations['Coefs'])\n",
        "    c += 1\n",
        "\n",
        "  sets = list(coefs_map.values())\n",
        "  first = sets[0]\n",
        "\n",
        "  for s in sets[1:]:\n",
        "    first = first.intersection(s)\n",
        "\n",
        "  return first"
      ],
      "metadata": {
        "id": "OvgogSudBSH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_coefs = extract_successful_combinations([nasdaq_dff, fpb_dff, sentfin_dff])"
      ],
      "metadata": {
        "id": "CuwYqIXHBSH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valid coefs additional processing"
      ],
      "metadata": {
        "id": "RGo8BX7DCO1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_coef_params(dfs, names, coefs):\n",
        "  merged_df = pd.DataFrame()\n",
        "\n",
        "  for df, name in zip(dfs, names):\n",
        "    full_dff = calc_accuracy_ratio(df, coefs)\n",
        "\n",
        "    merged_df = pd.concat([merged_df, full_dff], ignore_index = True)\n",
        "\n",
        "    full_dff.to_csv(f'/content/drive/MyDrive/finbert process/{name}/add_info_metrics_grid_search.csv', index=False)\n",
        "\n",
        "\n",
        "  merged_df.to_csv('/content/drive/MyDrive/finbert process/best_coefs_all_dfs_summary.csv', index=False)\n",
        "\n",
        "  return merged_df"
      ],
      "metadata": {
        "id": "ncskcyea4ja4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp0q3PzPArs_"
      },
      "outputs": [],
      "source": [
        "names = ['nasdaq', 'fpb', 'sentfin']\n",
        "dffs = []\n",
        "\n",
        "for n in names:\n",
        "  df = pd.read_csv(f'/content/drive/MyDrive/finbert process/{n}/raw results/summary_df.csv')\n",
        "  dffs.append(df)\n",
        "\n",
        "additional_params_df = find_best_coef_params(dffs, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best performing coefficients"
      ],
      "metadata": {
        "id": "osvjIH8jDAOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_xlex_lm_combs(dataset, metrics, agg_comb):\n",
        "  l = []\n",
        "\n",
        "  for comb in xlex_lm_combinations:\n",
        "    s = 0\n",
        "    for c in comb:\n",
        "      s += calc_for_metrics(dataset, metrics, f'{c} {agg_comb} {value_end}')\n",
        "\n",
        "    l.append(s)\n",
        "\n",
        "  return l\n",
        "\n",
        "def calc_for_metrics(dataset, metrics, comb):\n",
        "  s = 0\n",
        "\n",
        "  for metric in metrics:\n",
        "    for source in lexicon_sources:\n",
        "      s += dataset[(dataset['Lexicon Source'] == source) & (dataset['Metric'] == metric)][comb].values[0]\n",
        "\n",
        "  return s\n",
        "\n",
        "def calc_best_options(dataset):\n",
        "  list_values = []\n",
        "\n",
        "  for coef in coefs:\n",
        "    coef_str = ','.join(str(value) for value in coef)\n",
        "    coef_dataset = dataset[(dataset['C1'] == coef[0]) & (dataset['C2'] == coef[1]) & (dataset['C3'] == coef[2]) & (dataset['C4'] == coef[3])].copy(True)\n",
        "\n",
        "    for agg_comb in agg_combinations:\n",
        "      for metrics, metric_type in zip([accuracy_metrics, primary_metrics, all_metrics], ['Accuracy', 'Primary', 'All']):\n",
        "        xlex_lm_combs_res = calc_xlex_lm_combs(coef_dataset, metrics, agg_comb)\n",
        "        new_row = [coef_str, agg_comb, metric_type] + xlex_lm_combs_res\n",
        "        list_values.append(new_row)\n",
        "\n",
        "  columns = ['Coefs', 'Agg Comb', 'Metric Type', 'XLex + LM', 'Xlex,XLex + LM', 'Xlex on LM,Xlex,XLex + LM']\n",
        "\n",
        "  return pd.DataFrame(list_values, columns = columns)"
      ],
      "metadata": {
        "id": "xs0LLyjyDZ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "all_coefs_summary = pd.read_csv('/content/drive/MyDrive/finbert process/best_coefs_all_dfs_summary.csv')"
      ],
      "metadata": {
        "id": "QKAi8zeRjGLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefs = valid_coefs\n",
        "\n",
        "accuracy_metrics = ['Accuracy']\n",
        "primary_metrics = ['Accuracy', 'F1', 'MCC']\n",
        "all_metrics = primary_metrics + ['Precision', 'Recall']\n",
        "\n",
        "agg_combinations = ['Sum Abs', 'Sum Diff', 'Avg Abs', 'Avg Diff']\n",
        "value_end = 'Value'\n",
        "xlex_lm_combinations = [['XLex + LM', 'XLex + LM on LM'], ['XLex + LM', 'XLex + LM on LM', 'XLex'], ['XLex + LM', 'XLex + LM on LM', 'XLex', 'XLex on LM']]\n",
        "lexicon_sources = ['nasdaq', 'fpb', 'sentfin']\n",
        "\n",
        "\n",
        "new_df = calc_best_options(all_coefs_summary)"
      ],
      "metadata": {
        "id": "aZxLbrWHj9Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_types = ['Accuracy', 'Primary', 'All']\n",
        "agg_types = ['Sum Abs', 'Sum Diff', 'Avg Abs', 'Avg Diff']\n",
        "s_values = [['XLex + LM'], ['Xlex,XLex + LM'], ['Xlex on LM,Xlex,XLex + LM'], ['XLex + LM', 'Xlex,XLex + LM'], ['XLex + LM', 'Xlex,XLex + LM', 'Xlex on LM,Xlex,XLex + LM']]\n",
        "\n",
        "coef_map = {}\n",
        "\n",
        "for s in s_values:\n",
        "  for agg in agg_types:\n",
        "    for metr in metric_types:\n",
        "      comb = new_df[(new_df['Metric Type'] == metr) & (new_df['Agg Comb'] == agg)].sort_values(by = s, ascending=[False] * len(s)).head(1).values[0][0]\n",
        "      if comb in coef_map:\n",
        "        coef_map[comb] = coef_map[comb] + 1\n",
        "      else:\n",
        "        coef_map[comb] = 1"
      ],
      "metadata": {
        "id": "Bl2luZoTnWQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of occurence of each coefficient combination as the superior one for the different types of result combinations\n",
        "coef_map"
      ],
      "metadata": {
        "id": "lq13_D_ppaJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1vYsMXK4-qsJ25mU2RVqY8ZusC3tY6m8W",
      "authorship_tag": "ABX9TyMlFTAZausx+JM/6dEd7NDz"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}