{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User input"
      ],
      "metadata": {
        "id": "K6HIf0rycoYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This session should be connected to GPU"
      ],
      "metadata": {
        "id": "6Xb023AKLsfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRmThHDwv7bj"
      },
      "outputs": [],
      "source": [
        "# enter the location of the sentiment clasification model\n",
        "model_loc = '/content/drive/MyDrive/roberta_model'\n",
        "\n",
        "# enter the location of the tokenizer\n",
        "tokenizer_loc = '/content/drive/MyDrive/roberta_tokenizer'\n",
        "\n",
        "# enter the source dataset location\n",
        "source_dataset_loc = '/content/drive/MyDrive/nasdaq.csv'\n",
        "\n",
        "# enter the folder where all subsequent datasets will be saved\n",
        "lexicon_folder_loc = '/content/drive/MyDrive/nasdaq'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTH2rxnizvGF"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVdPC3rlpl1j",
        "outputId": "bee3b322-2850-413b-bdcb-a9ba3a9c5bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.10.2\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (2.27.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (2022.10.31)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.10.2) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.10.2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.10.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.10.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.10.2) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.10.2) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.10.2) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.10.2) (1.1.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=d5063b2da81e85af4d1159c9a898630258db5e96717f938a0218e813f49034c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.10.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYv0LgBIj1j3",
        "outputId": "823a7323-aa1e-43fa-e32d-480e162158fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (0.0.53)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (1.22.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (2.27.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (1.13.1+cu116)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.104-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch_transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.5.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.104\n",
            "  Downloading botocore-1.29.104-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_transformers) (1.26.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch_transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch_transformers) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.104->boto3->pytorch_transformers) (2.8.2)\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, pytorch_transformers\n",
            "Successfully installed boto3-1.26.104 botocore-1.29.104 jmespath-1.0.1 pytorch_transformers-1.2.0 s3transfer-0.6.0 sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M9EBnH7j7so",
        "outputId": "fca6a95b-7a91-4b15-8f70-e50a15f98149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.4/572.4 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from shap) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from shap) (1.4.4)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.9/dist-packages (from shap) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.9/dist-packages (from shap) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->shap) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->shap) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shortuuid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsnUdGQHj9KD",
        "outputId": "3214859c-caf2-41a1-aed0-4981da33bd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shortuuid\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: shortuuid\n",
            "Successfully installed shortuuid-1.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm3JN7cX9iV6"
      },
      "source": [
        "# Word Extraction Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBrMUFmWjvwO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "import shap\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "import shortuuid\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import preprocessor as p\n",
        "import gdown\n",
        "\n",
        "class WordExtractor:\n",
        "\n",
        "  def __init__(self, model, explainer, drive_loc):\n",
        "    self.model = model\n",
        "    self.explainer = explainer\n",
        "    \n",
        "    self.drive_loc = drive_loc if drive_loc[-1] == '/' else f'{drive_loc}/'\n",
        "    self.log_uuid = shortuuid.uuid()\n",
        "\n",
        "    self.negative_class = 0\n",
        "    self.positive_class = 1\n",
        "    self.score_col = 'score'\n",
        "\n",
        "    self.log_df_list = []\n",
        "    self.positive_words_df_list = []\n",
        "    self.negative_words_df_list = []\n",
        "\n",
        "    self.log_columns = ['uuid', 'sentence', 'positive_score', 'negative_score', 'label', 'values', \"base_values\", \"data\"]\n",
        "    self.pos_neg_columns = ['word', 'count', 'shap_values', 'sentence_uuid']\n",
        "\n",
        "    self.tz_utc = pytz.timezone('UTC')\n",
        "\n",
        "\n",
        "  def extracting_sentence_label(self, sentence):\n",
        "    # prediction\n",
        "    pred = self.model([sentence])[0]\n",
        "\n",
        "    # extraction poitive and negative score from prediction\n",
        "    neg_score = pred[self.negative_class][self.score_col]\n",
        "    pos_score = pred[self.positive_class][self.score_col]\n",
        "\n",
        "    # determining the label\n",
        "    label = self.negative_class if neg_score >= pos_score else self.positive_class\n",
        "\n",
        "    return sentence, label, neg_score, pos_score\n",
        "\n",
        "  def shap_explaining_results(self, sentence):\n",
        "    # shap explaining the prediction of the model\n",
        "    shap_values = self.explainer([sentence])\n",
        "\n",
        "    # extract data from the prediction\n",
        "    words = shap_values.data[0]\n",
        "    # extract shap values from the prediction\n",
        "    values = shap_values.values[0]\n",
        "    # extract shap base values from the prediction\n",
        "    base_value = shap_values.base_values[0]\n",
        "    return words, values, base_value\n",
        "\n",
        "\n",
        "  def log_item(self, uuid, sentence, pos_score, neg_score, label, words, values, base_value):\n",
        "    self.log_df_list.append([uuid, sentence, pos_score, neg_score, label, values, base_value, words])\n",
        "\n",
        "  def add_new_word(self, word, probability, dataset, uuid):\n",
        "    dataset.append([word, 1, str(probability), uuid])\n",
        "\n",
        "  def save_location(self, location, type_dataset, prefix):\n",
        "    if location == 'drive':\n",
        "      return f'{self.drive_loc}{self.log_uuid}--{prefix}{type_dataset}.csv'\n",
        "\n",
        "    return f'/content/{self.log_uuid}--{prefix}{type_dataset}.csv'\n",
        "\n",
        "  def create_df(self):\n",
        "    # create dataframes from the lists\n",
        "    log_df = pd.DataFrame(self.log_df_list, columns = self.log_columns)\n",
        "    positive_df = pd.DataFrame(self.positive_words_df_list, columns = self.pos_neg_columns)\n",
        "    negative_df = pd.DataFrame(self.negative_words_df_list, columns = self.pos_neg_columns)\n",
        "\n",
        "    return log_df, positive_df, negative_df\n",
        "\n",
        "  def save_datasets(self, prefix='tmp_', location = None):\n",
        "    log_df, positive_df, negative_df = self.create_df()\n",
        "\n",
        "    log_df.to_csv(self.save_location(location, 'log_dataset', prefix), index=False)\n",
        "    positive_df.to_csv(self.save_location(location, 'positive_words', prefix), index=False)\n",
        "    negative_df.to_csv(self.save_location(location, 'negative_words', prefix), index=False)\n",
        "\n",
        "    return log_df, positive_df, negative_df\n",
        "\n",
        "\n",
        "  def print_log(self, count):\n",
        "    # loging in utc time zone\n",
        "    print('Printing line number ' + str(count))\n",
        "    datetime_utc = datetime.now(self.tz_utc)\n",
        "    print(\"Current time:\", datetime_utc.strftime(\"%H:%M:%S\"))\n",
        "    self.save_datasets()\n",
        "\n",
        "  def classify_word(self, word, shap_value, label, uuid):\n",
        "    word = str.lower(word)\n",
        "    if label == self.negative_class:\n",
        "      # if the score that is from the predicted label (negative) is greater or equal to 0, then add the words to the negative dataset, else add the same value, but abs to the positive dataset\n",
        "      if shap_value[label] >= 0:\n",
        "        self.add_new_word(word, shap_value[label], self.negative_words_df_list, uuid)\n",
        "      else:\n",
        "        self.add_new_word(word, abs(shap_value[label]), self.positive_words_df_list, uuid)\n",
        "\n",
        "    else:\n",
        "      # if the score that is from the predicted label (positive) is greater or equal to 0, then add the words to the positive dataset, else add the same value, but abs to the negative dataset\n",
        "      if shap_value[label] >= 0:\n",
        "        self.add_new_word(word, shap_value[label], self.positive_words_df_list, uuid)\n",
        "      else:\n",
        "        self.add_new_word(word, abs(shap_value[label]), self.negative_words_df_list, uuid)\n",
        "\n",
        "  def execute_process(self, sentence):\n",
        "    # creating sentence uuids that are added to the words extracted from that sentence\n",
        "    # this way we can debug and identify the origin sentences of one word in the dataset\n",
        "    uuid = shortuuid.uuid()\n",
        "\n",
        "    # log the progress on each 100 sentences\n",
        "    # save the datasets on each 100 sentences\n",
        "    # saving partial datasets in case when the session on Google Colab breaks\n",
        "    count = len(self.log_df_list)\n",
        "    if count % 100 == 0:\n",
        "      self.print_log(count)\n",
        "      self.save_datasets(location = 'drive')\n",
        "\n",
        "    # extracting the label, probability for the negative label and probability for the positive label\n",
        "    sentence, label, neg_score, pos_score = self.extracting_sentence_label(sentence)\n",
        "\n",
        "    # extract words, shap values and base values from SHAP explainer\n",
        "    words, values, base_value = self.shap_explaining_results(sentence)\n",
        "\n",
        "    # log the progress\n",
        "    self.log_item(uuid, sentence, pos_score, neg_score, label, words, values, base_value)\n",
        "\n",
        "    # classify the extracted words\n",
        "    [self.classify_word(word, shap_value, label, uuid) for word, shap_value in zip(words, values)]\n",
        "\n",
        "\n",
        "  def run_extraction(self, df, text_column):\n",
        "    start_number_point = self.extract_start_point()\n",
        "    sentences_left = len(df) - start_number_point\n",
        "    if sentences_left == 0:\n",
        "      raise Exception('No more sentences left for processing. Please continue with the next steps')\n",
        "    else:\n",
        "      print(f'{sentences_left} sentences are left for processing')\n",
        "\n",
        "    df[start_number_point:][text_column].apply(lambda sentence: self.execute_process(sentence))\n",
        "\n",
        "    self.save_datasets(prefix = '', location = 'drive')\n",
        "    return self.save_datasets('')\n",
        "\n",
        "\n",
        "  def extract_rows_number_from_execution(self, exec, files_locations):\n",
        "    exec_files = [f for f in files_locations if exec in f]\n",
        "    if len(exec_files) > 3:\n",
        "      final_files = [f for f in exec_files if 'tmp' not in f]\n",
        "    else:\n",
        "      final_files = exec_files.copy(True)\n",
        "\n",
        "    log_file = [f for f in final_files if 'log' in f][0]\n",
        "    log_df = pd.read_csv(log_file)\n",
        "    return len(log_df)\n",
        "\n",
        "  def extract_start_point(self):\n",
        "    files_locations = [join(self.drive_loc, f) for f in listdir(self.drive_loc) if isfile(join(self.drive_loc, f))]\n",
        "    if len(files_locations) == 0:\n",
        "      return 0\n",
        "\n",
        "    executions = set([f.split('/')[-1].split(sep='--')[0] for f in files_locations])\n",
        "    start_number_point = np.array([self.extract_rows_number_from_execution(exec, files_locations) for exec in executions]).sum()\n",
        "\n",
        "    return start_number_point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNJfk-sC9qRT"
      },
      "source": [
        "# Explainer creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaeQMGOUAvrt"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "#loading model and tokenizer from drive\n",
        "model = torch.load(model_loc)\n",
        "tokenizer = torch.load(tokenizer_loc)\n",
        "\n",
        "# For using it with CPU\n",
        "# model = torch.load(model_loc, map_location=torch.device('cpu'))\n",
        "# tokenizer = torch.load(tokenizer_loc, map_location=torch.device('cpu'))\n",
        "\n",
        "# creating pipeline for sentiment analysis\n",
        "md = pipeline('sentiment-analysis', model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
        "\n",
        "# For using it with CPU\n",
        "# md = pipeline('sentiment-analysis', model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "# setting sigmoid as activation function \n",
        "md.function_to_apply = 'sigmoid'\n",
        "\n",
        "# creating shap transformer pipeleni\n",
        "shap_p = shap.models.TransformersPipeline(md, rescale_to_logits=False)\n",
        "# creating shap explainer\n",
        "explainer = shap.Explainer(shap_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Words Collection"
      ],
      "metadata": {
        "id": "3Cr8nL_LdUE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the source dataset\n",
        "df = pd.read_csv(source_dataset_loc)\n",
        "\n",
        "# creating the word extractor object\n",
        "word_extractor = WordExtractor(md, explainer, lexicon_folder_loc)\n",
        "# extracting the words\n",
        "log_df, positive_df, negative_df = word_extractor.run_extraction(df, 'text')"
      ],
      "metadata": {
        "id": "7TrtjWq8dWPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}