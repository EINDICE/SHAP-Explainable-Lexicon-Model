{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMjZzmB6PTm"
      },
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twAm5QQn6PJG"
      },
      "outputs": [],
      "source": [
        "# enter the location of lexicons (please make sure that there are only lexicons files in the folder)\n",
        "lexicons_loc_map = {\n",
        "    'nasdaq': '/content/drive/MyDrive/nasdaq/lexicons',\n",
        "    'fpb': '/content/drive/MyDrive/fpb/lexicons',\n",
        "    'sentfin': '/content/drive/MyDrive/sentfin/lexicons'\n",
        "}\n",
        "\n",
        "# enter the folder location where the result dataset should be saved\n",
        "time_evaluation_df_loc = '/content/drive/MyDrive/'\n",
        "\n",
        "\n",
        "# enter the location of the tokenizer\n",
        "tokenizer_loc = '/content/drive/MyDrive/roberta_tokenizer'\n",
        "\n",
        "# enter the location of all evaluation datasets (please make sure that there are only evaluation files in the folder)\n",
        "eval_datasets_folder_loc = '/content/drive/MyDrive/datasets/evaluation datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8tFPSCnYtEv"
      },
      "source": [
        "# ShapDictModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOLcHcCZL6F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqVEyjNMQvwG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import nltk\n",
        "import torch\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ShapDictModel:\n",
        "  def __init__(self, tm_words, lm_words, tokenizer, word_column, category_column, decision_makers, count_column):\n",
        "    self.dataset_source = 'both'\n",
        "    self.word_column = word_column\n",
        "    self.tokenizer = tokenizer\n",
        "    self.count_column = count_column\n",
        "\n",
        "    # Lemmatizer\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # source column values\n",
        "    self.lm_source = 'LM'\n",
        "    self.tm_source = 'OUR_WORDS'\n",
        "\n",
        "    # column prefix\n",
        "    self.lm_prefix = 'LM_'\n",
        "    self.tm_prefix = 'TM_'\n",
        "\n",
        "    # category values\n",
        "    self.positive_category_value = 'positive'\n",
        "    self.negative_category_value = 'negative'\n",
        "\n",
        "    # opposite prefix\n",
        "    self.opposite_prefix = 'opposite_'\n",
        "\n",
        "    # prefix when one source is chosen\n",
        "    self.prefix = ''\n",
        "\n",
        "    # source column postfix\n",
        "    self.source_column = 'src'\n",
        "\n",
        "    # dataset on which results are calculated\n",
        "    self.tm_words = tm_words\n",
        "    self.lm_words = lm_words\n",
        "\n",
        "    # function that will calculate the score\n",
        "    self.calculate_score = self.calculate_score_both_dataset_sources\n",
        "\n",
        "    # category when one source is chosen\n",
        "    self.category = category_column\n",
        "\n",
        "    # decision makers\n",
        "    self.decision_makers = decision_makers\n",
        "\n",
        "  def calculate_dataset_source_score(self, words):\n",
        "\n",
        "    word_category = words[0]\n",
        "\n",
        "    selected_category_sign = 1\n",
        "    opposite_category_sign = 1\n",
        "    if word_category == self.positive_category_value:\n",
        "      opposite_category_sign = -1\n",
        "    elif word_category == self.negative_category_value:\n",
        "      selected_category_sign = -1\n",
        "\n",
        "    if selected_category_sign == opposite_category_sign:\n",
        "      return 0, 0\n",
        "\n",
        "    selected_category_score = words[1] * selected_category_sign\n",
        "    opposite_category_score = words[2] * opposite_category_sign\n",
        "\n",
        "    return selected_category_score, opposite_category_score\n",
        "\n",
        "  def calculate_score_both_dataset_sources(self, word, coefficients):\n",
        "    tm_accumulated_score, tm_opposite_accumulated_score = self.calculate_dataset_source_score(self.tm_words[word])\n",
        "    lm_accumulated_score, lm_opposite_accumulated_score = self.calculate_dataset_source_score(self.lm_words[word])\n",
        "\n",
        "    c1, c2, c3, c4 = coefficients\n",
        "\n",
        "    weighted_score = (c1 * tm_accumulated_score + c2 * tm_opposite_accumulated_score + c3 * lm_accumulated_score + c4 * lm_opposite_accumulated_score)\n",
        "\n",
        "    return weighted_score\n",
        "\n",
        "  def decide(self, word, coefficients):\n",
        "    if word not in self.tm_words or word not in self.lm_words:\n",
        "      return 0\n",
        "\n",
        "    return self.calculate_score(word, coefficients)\n",
        "\n",
        "  def predict_sentence_label(self, sentence, label_t, coefficients):\n",
        "    words = self.tokenizer.tokenize(sentence)\n",
        "    cleaned_words = self.clean_and_lemmatize_words(words)\n",
        "    decision_score = sum([self.decide(word, coefficients) for word in cleaned_words])\n",
        "\n",
        "    return 1 if decision_score > 0 else 0 if decision_score < 0 else -1\n",
        "\n",
        "  def __get_wordnet_pos(self, word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  def clean_and_lemmatize_words(self, words):\n",
        "    lower_case_words = [str(word).lower().replace('Ä¡', '').strip() for word in words]\n",
        "    return [self.lemmatizer.lemmatize(word, self.__get_wordnet_pos(word)) for word in lower_case_words]\n",
        "\n",
        "  def predict_and_evaluate(self, sentences, true_labels, coefficients):\n",
        "    predicted_labels = [self.predict_sentence_label(sentence, label, coefficients) for sentence, label in zip(sentences, true_labels)]\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def extract_file_name(file_loc):\n",
        "  return file_loc.split('/')[-1].split('.')[0]\n",
        "\n",
        "def extract_datasets_map(datasets_location):\n",
        "  location = datasets_location if datasets_location[-1] == '/' else f'{datasets_location}/'\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from: {location} ...')\n",
        "\n",
        "  assert files_locations != 0, 'No files found in the provided location'\n",
        "\n",
        "  datasets_map = {}\n",
        "  for f in files_locations:\n",
        "    print(f'Reading dataset: {f} ...')\n",
        "    dataset = pd.read_csv(f)\n",
        "    datasets_map[extract_file_name(f)] = dataset\n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "\n",
        "  return datasets_map\n",
        "\n",
        "\n",
        "def create_results_folder(loc):\n",
        "  parent_location = os.path.abspath(os.path.join(loc, os.pardir))\n",
        "  mod_location = parent_location if parent_location[-1] == '/' else f'{parent_location}/'\n",
        "\n",
        "  results_location = f'{mod_location}results'\n",
        "\n",
        "  if not os.path.exists(results_location):\n",
        "    os.makedirs(results_location)\n",
        "\n",
        "  print(f'Created results dataset on location: {results_location} ...')\n",
        "\n",
        "  return results_location"
      ],
      "metadata": {
        "id": "MeYjKqecOOvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Evaluation"
      ],
      "metadata": {
        "id": "PJon-ZctHPqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dicts(prefix, dataset):\n",
        "  # extracting only the necessary columns included in the decision making process\n",
        "  source_dict = {}\n",
        "  for item in dataset[['word', f'{prefix}_category', f'{prefix}_average_shap_values', f'{prefix}_opposite_average_shap_values']].values:\n",
        "    source_dict[item[0]] = item[1:]\n",
        "\n",
        "  return source_dict"
      ],
      "metadata": {
        "id": "8J-QOg4NHWX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating lexicons execution time\n",
        "\n",
        "import time\n",
        "import timeit\n",
        "\n",
        "def run_model():\n",
        "  return shap_dict_model.predict_and_evaluate(sentences, labels, [0.8, 0.2, 0.9, 0.5])\n",
        "\n",
        "eval_datasets_map = extract_datasets_map(eval_datasets_folder_loc)\n",
        "tokenizer = torch.load(tokenizer_loc)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for src in lexicons_loc_map:\n",
        "  lexicon_loc = lexicons_loc_map[src]\n",
        "  lexicon_map = extract_datasets_map(lexicon_loc)\n",
        "\n",
        "  for lex in lexicon_map:\n",
        "\n",
        "    df = lexicon_map[lex]\n",
        "    tm_dict = create_dicts('TM', df)\n",
        "    lm_dict = create_dicts('LM', df)\n",
        "\n",
        "    for eval_name in eval_datasets_map:\n",
        "      if src == 'fpb' and eval_name == 'financial_phrase_bank':\n",
        "        continue\n",
        "\n",
        "      eval_df = eval_datasets_map[eval_name]\n",
        "      sentences = eval_df.text.values\n",
        "      labels = eval_df.sentiment.values\n",
        "\n",
        "      shap_dict_model = ShapDictModel(tm_dict, lm_dict, tokenizer, 'word', 'category', ['average_shap_values'], 'count')\n",
        "\n",
        "      num_times = 10\n",
        "      execution_time = timeit.timeit(run_model, number=num_times)\n",
        "      duration = execution_time / num_times\n",
        "\n",
        "      lex_num = len(df)\n",
        "      sent_num = len(sentences)\n",
        "      normalized = 'normalized' in lex\n",
        "      new_row = [src, normalized, lex_num, eval_name, sent_num, duration]\n",
        "      rows.append(new_row)\n",
        "\n",
        "cols = ['Source Lexicon', 'Normalized', 'Lexicon No. Words', 'Eval Dataset', 'Sentences No.', 'Time in s']\n",
        "\n",
        "time_df = pd.DataFrame(rows, columns = cols)\n",
        "\n",
        "time_df_loc = f'{time_evaluation_df_loc}/roberta_lexicons_time_evaluation_average_10_times.csv'\n",
        "time_df.to_csv(time_df_loc, index=False)"
      ],
      "metadata": {
        "id": "yRPeyDVAYDoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IeKUL4fpGunhLMUbOgWuWMLsZLwYUebM",
      "authorship_tag": "ABX9TyOSTFNglSxz9GRBpxfpK/XM"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}