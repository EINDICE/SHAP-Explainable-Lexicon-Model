{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hristijanpeshov/SHAP-Explainable-Lexicon-Model/blob/master/model_evaluation_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMjZzmB6PTm"
      },
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twAm5QQn6PJG"
      },
      "outputs": [],
      "source": [
        "# enter source lexicon name (this will appear in the results dataset)\n",
        "lexicon_name = 'nasdaq'\n",
        "\n",
        "# enter the location of lexicons (please make sure that there are only lexicons files in the folder)\n",
        "lexicons_folder_loc = '/content/drive/MyDrive/nasdaq/concatenated datasets/lexicons'\n",
        "\n",
        "# enter the location of tokenizer\n",
        "tokenizer_loc = '/content/drive/MyDrive/roberta/roberta_tokenizer'\n",
        "\n",
        "# enter the location of all evaluation datasets (please make sure that there are only evaluation files in the folder)\n",
        "eval_datasets_folder_loc = '/content/drive/MyDrive/datasets/evaluation datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8tFPSCnYtEv"
      },
      "source": [
        "# ShapDictModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOLcHcCZL6F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqVEyjNMQvwG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import nltk\n",
        "import torch\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ShapDictModel:\n",
        "  def __init__(self, dataset, tokenizer, word_column, category_column, decision_makers, count_column, dataset_source = None):\n",
        "    self.dataset_source = str.upper(dataset_source) if dataset_source is not None else 'both'\n",
        "    self.word_column = word_column\n",
        "    self.tokenizer = tokenizer\n",
        "    self.count_column = count_column\n",
        "\n",
        "    # Lemmatizer\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # source column values\n",
        "    self.lm_source = 'LM'\n",
        "    self.tm_source = 'OUR_WORDS'\n",
        "\n",
        "    # column prefix\n",
        "    self.lm_prefix = 'LM_'\n",
        "    self.tm_prefix = 'TM_'\n",
        "\n",
        "    # category values\n",
        "    self.positive_category_value = 'positive'\n",
        "    self.negative_category_value = 'negative'\n",
        "\n",
        "    # opposite prefix\n",
        "    self.opposite_prefix = 'opposite_'\n",
        "\n",
        "    # number of required coefficient when both sources or one sorce\n",
        "    self.coefficient_number_both_sources = 4\n",
        "    self.coefficient_number_one_source = 2\n",
        "\n",
        "    # prefix when one source is chosen\n",
        "    self.prefix = 'TM_' if self.dataset_source == self.tm_source else 'LM_' if self.dataset_source == self.lm_source else ''\n",
        "\n",
        "    # source column postfix\n",
        "    self.source_column = 'src'\n",
        "\n",
        "    # dataset on which results are calculated\n",
        "    self.dataset = dataset if self.dataset_source == 'both' else self.extract_dataset_from_source(dataset, self.dataset_source)\n",
        "\n",
        "    # function that will calculate the score\n",
        "    self.calculate_score = self.calculate_score_both_dataset_sources if self.dataset_source == 'both' else self.calculate_score_one_dataset_source\n",
        "\n",
        "    # category when one source is chosen\n",
        "    self.category = category_column\n",
        "\n",
        "    # decision makers\n",
        "    self.decision_makers = decision_makers\n",
        "    print()\n",
        "    print(f'Created ShapDictModel with decision makers: {self.decision_makers}')\n",
        "    print()\n",
        "\n",
        "\n",
        "  def extract_dataset_from_source(self, dataset, source, only_source_columns=True):\n",
        "    # depending on which source is chosen, the full dataset will be modified to return the required dataset\n",
        "\n",
        "    # if LM is chosen as source, then the returned dataset will contain only the words that were originally from the LM dataset with LM_ added as prefix to the columns\n",
        "    if source == self.lm_source:\n",
        "      prefix = self.lm_prefix\n",
        "      column = f'{self.lm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.tm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.lm_source\n",
        "      opposite_dataset_source = self.tm_source\n",
        "    else:\n",
        "      # if OUR_WORDS is chosen as source, then the returned dataset will contain only the words that were originally from our words dataset with TM_ added as prefix to the columns\n",
        "      prefix = self.tm_prefix\n",
        "      column = f'{self.tm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.lm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.tm_source\n",
        "      opposite_dataset_source = self.lm_source\n",
        "\n",
        "    source_dataset = dataset[(dataset[column] == dataset_source) & ((dataset[opposite_column] == dataset_source) | (dataset[opposite_column] == opposite_dataset_source))]\n",
        "\n",
        "    # filtering so just the necessary columns will remain\n",
        "    if only_source_columns:\n",
        "      columns = list(source_dataset.columns)\n",
        "      source_columns = [self.word_column] + [column for column in columns if prefix in column]\n",
        "\n",
        "      return source_dataset[source_columns]\n",
        "\n",
        "    return source_dataset\n",
        "\n",
        "  def calculate_score_both_dataset_sources(self, word_occurence, coefficients):\n",
        "    tm_accumulated_score, tm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.tm_prefix)\n",
        "    lm_accumulated_score, lm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.lm_prefix)\n",
        "\n",
        "    c1, c2, c3, c4 = coefficients\n",
        "\n",
        "    weighted_score = (c1 * tm_accumulated_score + c2 * tm_opposite_accumulated_score + c3 * lm_accumulated_score + c4 * lm_opposite_accumulated_score)\n",
        "\n",
        "    return weighted_score\n",
        "\n",
        "  def calculate_score_one_dataset_source(self, word_occurence, coefficients):\n",
        "    accumulated_score, opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.prefix)\n",
        "\n",
        "    c1, c2 = coefficients\n",
        "\n",
        "    weighted_score = (c1 * accumulated_score + c2 * opposite_accumulated_score)\n",
        "\n",
        "    return weighted_score\n",
        "\n",
        "  def predict_sentence_label(self, sentence, label_t, coefficients):\n",
        "    words = self.tokenizer.tokenize(sentence)\n",
        "    cleaned_words = self.clean_and_lemmatize_words(words)\n",
        "\n",
        "    decision_score = 0\n",
        "    for word in cleaned_words:\n",
        "      # it can only have one occurence, but to check if it occurs at all\n",
        "      word_occurences = self.dataset.loc[self.dataset[self.word_column] == word].values\n",
        "      if len(word_occurences) == 0:\n",
        "        continue\n",
        "\n",
        "      word_occurence = word_occurences[0]\n",
        "\n",
        "      decision_score += self.calculate_score(word_occurence, coefficients)\n",
        "\n",
        "    label = 1 if decision_score > 0 else 0 if decision_score < 0 else -1\n",
        "\n",
        "    return label\n",
        "\n",
        "  def __get_wordnet_pos(self, word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  def clean_and_lemmatize_words(self, words):\n",
        "    lower_case_words = [str(word).lower().replace('Ä¡', '').strip() for word in words]\n",
        "    return [self.lemmatizer.lemmatize(word, self.__get_wordnet_pos(word)) for word in lower_case_words]\n",
        "\n",
        "  def calculate_word_dm_score(self, word_occurence, decision_maker_column, count_column):\n",
        "    columns = list(self.dataset.columns)\n",
        "\n",
        "    decision_maker_index = columns.index(decision_maker_column)\n",
        "\n",
        "    value = word_occurence[decision_maker_index]\n",
        "\n",
        "    return value\n",
        "\n",
        "  def calculate_dataset_source_score(self, word_occurence, column_prefix):\n",
        "    columns = list(self.dataset.columns)\n",
        "    category_index = columns.index(f'{column_prefix}{self.category}')\n",
        "\n",
        "    word_category = word_occurence[category_index]\n",
        "\n",
        "    selected_category_sign = 1\n",
        "    opposite_category_sign = 1\n",
        "    if word_category == self.positive_category_value:\n",
        "      opposite_category_sign = -1\n",
        "    elif word_category == self.negative_category_value:\n",
        "      selected_category_sign = -1\n",
        "\n",
        "    if selected_category_sign == opposite_category_sign:\n",
        "      return 0, 0\n",
        "\n",
        "    opposite_column_prefix = f'{column_prefix}{self.opposite_prefix}'\n",
        "\n",
        "    selected_category_score = 0\n",
        "    opposite_category_score = 0\n",
        "    for decision_maker in self.decision_makers:\n",
        "      selected_category_score += self.calculate_word_dm_score(word_occurence, f'{column_prefix}{decision_maker}', f'{column_prefix}{self.count_column}') * selected_category_sign\n",
        "      opposite_category_score += self.calculate_word_dm_score(word_occurence, f'{opposite_column_prefix}{decision_maker}', f'{opposite_column_prefix}{self.count_column}') * opposite_category_sign\n",
        "\n",
        "    return selected_category_score, opposite_category_score\n",
        "\n",
        "  def calculate_model_accuracy(self, true_labels, predicted_labels):\n",
        "    accuracy_indicators = [true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)]\n",
        "\n",
        "    return np.asarray(accuracy_indicators).sum() / len(true_labels)\n",
        "\n",
        "\n",
        "  def predict_and_evaluate(self, sentences, true_labels, coefficients):\n",
        "    if self.dataset_source == 'both':\n",
        "      assert len(coefficients) == self.coefficient_number_both_sources, f'{self.coefficient_number_both_sources} coefficients required, provided {len(coefficients)}'\n",
        "    else:\n",
        "      assert len(coefficients) == self.coefficient_number_one_source, f'{self.coefficient_number_one_source} coefficients required, provided {len(coefficients)}'\n",
        "\n",
        "    predicted_labels = [self.predict_sentence_label(sentence, label, coefficients) for sentence, label in zip(sentences, true_labels)]\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "      our_acc = self.calculate_model_accuracy(true_labels, predicted_labels)\n",
        "      print(f'Our method accuracy score: {our_acc}')\n",
        "\n",
        "      acc = accuracy_score(true_labels, predicted_labels)\n",
        "      print(f'Accuracy score: {acc}')\n",
        "      results.append(acc)\n",
        "\n",
        "      pr = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "      print(f'Precision score: {pr}')\n",
        "      results.append(pr)\n",
        "\n",
        "      rec = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "      print(f'Recall score: {rec}')\n",
        "      results.append(rec)\n",
        "\n",
        "      f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "      print(f'F1 score: {f1}')\n",
        "      results.append(f1)\n",
        "\n",
        "      mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
        "      print(f'MCC score: {mcc}')\n",
        "      results.append(mcc)\n",
        "\n",
        "      print()\n",
        "      print(\"Classification Report:\")\n",
        "      cl_report = classification_report(true_labels, predicted_labels, zero_division=0)\n",
        "      print(cl_report)\n",
        "\n",
        "      print()\n",
        "      print(\"Confusion Matrix:\")\n",
        "      conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "      print(conf_matrix)\n",
        "\n",
        "    except:\n",
        "      print('Error while trying to calculate metrics')\n",
        "      none_list = [None] * (6 - len(results))\n",
        "      final_result = results + none_list\n",
        "\n",
        "      our_acc, acc_final, pr_final, rec_final, f1_final, mcc_final = final_result\n",
        "      return predicted_labels, final_result, {'Our Accuracy': our_acc, 'Accuracy': acc_final, 'Precision': pr_final, 'Recall': rec_final, 'F1': f1_final, 'MCC': mcc_final}, None, None\n",
        "\n",
        "    # return predicted_labels\n",
        "\n",
        "    # just for creating of the results dataset\n",
        "    acc_final, pr_final, rec_final, f1_final, mcc_final = results\n",
        "    return predicted_labels, results, {'Accuracy': acc_final, 'Precision': pr_final, 'Recall': rec_final, 'F1': f1_final, 'MCC': mcc_final}, cl_report, conf_matrix\n",
        "\n",
        "\n",
        "  def __normalize_column(self, dataset, column):\n",
        "    column_max_value = dataset[column].max()\n",
        "    if column_max_value == 0:\n",
        "      return dataset\n",
        "\n",
        "    dataset[column] = dataset[column].apply(lambda value: value / column_max_value)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def normalize_dataset(self, dataset):\n",
        "    columns_to_normalize = dataset.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    print(f'Columns to normalize: {columns_to_normalize}')\n",
        "\n",
        "    dataset_copy = dataset.copy(True)\n",
        "    modified_datasets = [self.__normalize_column(dataset_copy, column) for column in columns_to_normalize]\n",
        "\n",
        "    print('Columns normalized')\n",
        "\n",
        "    return modified_datasets[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cr2cEKCYDY4"
      },
      "outputs": [],
      "source": [
        "def convert_to_num(data, column):\n",
        "  sentiment_map = {\n",
        "      'positive': 1,\n",
        "      'negative': 0\n",
        "  }\n",
        "  return data[column].apply(lambda s: sentiment_map[s]).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS3kUmiCdvym"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pytz\n",
        "\n",
        "def create_summary_results(lexicon_source, lexicon_datasets, evaluation_datasets, drive_loc):\n",
        "\n",
        "  evaluation_summary = []\n",
        "  evaluation_summary_one_line = []\n",
        "  for lexicon_name in lexicon_datasets:\n",
        "    lexicon = lexicon_datasets[lexicon_name]\n",
        "    lexicon_normalized = 'normalized' in lexicon_name\n",
        "\n",
        "    for evaluate_dataset_name in evaluation_datasets:\n",
        "      evaluate_dataset = evaluation_datasets[evaluate_dataset_name]\n",
        "      sentences = evaluate_dataset['text'].values\n",
        "      true_labels = evaluate_dataset['sentiment'].values\n",
        "      coefs_our_words = [0.8, 0.2]\n",
        "      coefs_lmd = [0.9, 0.5]\n",
        "      coefs = coefs_our_words + coefs_lmd\n",
        "\n",
        "      dataset = lexicon.copy(True)\n",
        "\n",
        "      lexicon_type = 'normalized' if lexicon_normalized else 'merged'\n",
        "      log_name = f'{lexicon_source}-{lexicon_type}-{evaluate_dataset_name}'\n",
        "      log_location = f'{drive_loc}/{log_name}.txt'\n",
        "      f = open(log_location, 'w')\n",
        "\n",
        "      evalutaion_result, evalutaion_result_single_line = evaluate(dataset, sentences, true_labels, coefs, coefs_our_words, coefs_lmd, lexicon_source, lexicon_normalized, evaluate_dataset_name, log_location, f)\n",
        "      evaluation_summary = evaluation_summary + evalutaion_result\n",
        "      evaluation_summary_one_line.append(evalutaion_result_single_line)\n",
        "\n",
        "      f.close()\n",
        "\n",
        "\n",
        "  evaluation_summary_df = pd.DataFrame(evaluation_summary, columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Words Source', 'Decision Maker', 'C1', 'C2', 'C3', 'C4',\n",
        "                        'Accuracy',\t'Precision', 'Recall', 'F1', 'MCC', 'Log'])\n",
        "  evaluation_summary_one_line_df = pd.DataFrame(evaluation_summary_one_line, columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Decision Maker', 'C1', 'C2', 'C3', 'C4',\n",
        "                        'OUR + LMD',\t'OUR', 'LMD', 'LMD on LMD', 'OUR on LMD', 'OUR + LMD on LMD', 'Log'])\n",
        "\n",
        "  evaluation_summary_df.to_csv(f'{drive_loc}/summary_df.csv', index=False)\n",
        "  evaluation_summary_one_line_df.to_csv(f'{drive_loc}/summary_one_line_df.csv', index=False)\n",
        "\n",
        "  return evaluation_summary_df, evaluation_summary_one_line_df\n",
        "\n",
        "def create_repetition_string(repetitive_string, times, split):\n",
        "  repetition = [repetitive_string] * times\n",
        "\n",
        "  return f\"{split}\".join(repetition)\n",
        "\n",
        "def write_result_to_file(f, execution_number, results_dictionary, classification_report, confussion_matrix):\n",
        "  f.write(f'Execution number: {execution_number}')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "\n",
        "  [f.write(f'{metric}: {results_dictionary[metric]}\\n') for metric in results_dictionary]\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 3, ''))\n",
        "\n",
        "  f.write(\"Classification Report:\\n\")\n",
        "  f.write(classification_report)\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "\n",
        "  f.write(\"Confusion Matrix:\\n\")\n",
        "  f.write(str(confussion_matrix))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 3, ''))\n",
        "  f.write(create_repetition_string('-', 70, ''))\n",
        "  f.write(create_repetition_string('\\n', 4, ''))\n",
        "\n",
        "def end_of_section(f):\n",
        "  for i in range (4):\n",
        "    f.write(create_repetition_string('-', 70, ''))\n",
        "    f.write(create_repetition_string('\\n', 1, ''))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "\n",
        "def create_had_responses_df(sentences, predicted_labels, true_labels):\n",
        "  lm_responses_df = pd.DataFrame(list(zip(sentences, predicted_labels, true_labels)), columns = ['sentences', 'predicted_label', 'true_label'])\n",
        "\n",
        "  had_responses_df = lm_responses_df[lm_responses_df['predicted_label'] != -1]\n",
        "\n",
        "  return had_responses_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lswiN64QfQP"
      },
      "outputs": [],
      "source": [
        "def run_shap_dict_model(dataset, sentences, true_labels, coefs, dataset_source = None):\n",
        "  shap_dict_model = ShapDictModel(dataset, tokenizer, 'word', 'category', ['average_shap_values'], 'count', dataset_source = dataset_source)\n",
        "  result = shap_dict_model.predict_and_evaluate(sentences, true_labels, coefs)\n",
        "\n",
        "  return result\n",
        "\n",
        "def evaluation_result(result, coefs, lexicon_source, lexicon_name, evaluate_dataset_name, words_source, log_location, dataset_source = None):\n",
        "  _, metrics_list, metrics_dict, cl_report, conf_matrix = result\n",
        "\n",
        "  if dataset_source == 'OUR_WORDS':\n",
        "    new_coefs = coefs + ['\\\\', '\\\\']\n",
        "  elif dataset_source == 'LM':\n",
        "    new_coefs = ['\\\\', '\\\\'] + coefs\n",
        "  else:\n",
        "    new_coefs = coefs\n",
        "\n",
        "  return [lexicon_source, lexicon_name, evaluate_dataset_name, words_source, 'average_shap_values'] + new_coefs + metrics_list + [log_location]\n",
        "\n",
        "def get_accuracy(result):\n",
        "   _, metrics_list, metrics_dict, cl_report, conf_matrix = result\n",
        "\n",
        "   return metrics_dict['Accuracy']\n",
        "\n",
        "def evaluation_result_one_line(metrics_list, coefs, lexicon_source, lexicon_name, evaluate_dataset_name, log_location):\n",
        "  return [lexicon_source, lexicon_name, evaluate_dataset_name, 'average_shap_values'] + coefs + metrics_list + [log_location]\n",
        "\n",
        "\n",
        "def evaluate(dataset, sentences, true_labels, coefs, coefs_our_words, coefs_lmd, lexicon_source, lexicon_name, evaluate_dataset_name, log_location, f):\n",
        "  evaluation_results = []\n",
        "  accuracy_results = []\n",
        "  f.write('OUR WORDS + LMD')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  result = run_shap_dict_model(dataset, sentences, true_labels, coefs)\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs, lexicon_source, lexicon_name, evaluate_dataset_name, 'OUR + LMD', log_location))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  f.write('OUR WORDS')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  result = run_shap_dict_model(dataset, sentences, true_labels, coefs_our_words, dataset_source='OUR_WORDS')\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs_our_words, lexicon_source, lexicon_name, evaluate_dataset_name, 'OUR', log_location, dataset_source='OUR_WORDS'))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  f.write('LMD')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  result = run_shap_dict_model(dataset, sentences, true_labels, coefs_lmd, dataset_source='LM')\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs_lmd, lexicon_source, lexicon_name, evaluate_dataset_name, 'LMD', log_location, dataset_source='LM'))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "\n",
        "  had_responses_df = create_had_responses_df(sentences, result[0], true_labels)\n",
        "\n",
        "  f.write('LMD accuracy for those that LMD had answer for')\n",
        "  f.write(f\"LMD accuracy score: {accuracy_score(had_responses_df['true_label'].values, had_responses_df['predicted_label'].values)}\")\n",
        "  result = run_shap_dict_model(dataset, had_responses_df['sentences'].values, had_responses_df['true_label'].values, coefs_lmd, dataset_source='LM')\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs_lmd, lexicon_source, lexicon_name, evaluate_dataset_name, 'LMD on LMD', log_location, dataset_source='LM'))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  f.write('OUR WORDS accuracy for those that LMD had answer for')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  # prediction by using explainable words applied only on the sentences on which LMD had a prediction (there were words in the sentence that were also in LMD)\n",
        "  result = run_shap_dict_model(dataset, had_responses_df['sentences'].values, had_responses_df['true_label'].values, coefs_our_words, dataset_source='OUR_WORDS')\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs_our_words, lexicon_source, lexicon_name, evaluate_dataset_name, 'OUR on LMD', log_location, dataset_source='OUR_WORDS'))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  f.write('\\n')\n",
        "  f.write('OUR WORDS + LMD accuracy for those that LMD had answer for')\n",
        "  f.write(create_repetition_string('\\n', 2, ''))\n",
        "  # prediction by using explainable words + LMD words, applied again only on the sentences on which LMD had a prediction (there were words in the sentence that were also in LMD)\n",
        "  result = run_shap_dict_model(dataset, had_responses_df['sentences'].values, had_responses_df['true_label'].values, coefs)\n",
        "  f.write(str(result))\n",
        "  f.write('\\n')\n",
        "  evaluation_results.append(evaluation_result(result, coefs, lexicon_source, lexicon_name, evaluate_dataset_name, 'OUR + LMD on LMD', log_location))\n",
        "  accuracy_results.append(get_accuracy(result))\n",
        "\n",
        "  return evaluation_results, evaluation_result_one_line(accuracy_results, coefs, lexicon_source, lexicon_name, evaluate_dataset_name, log_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ZfUoZdNGPV"
      },
      "source": [
        "# Evaluate lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnm_ohd6wAy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def extract_file_name(file_loc):\n",
        "  return file_loc.split('/')[-1].split('.')[0]\n",
        "\n",
        "def extract_datasets_map(datasets_location):\n",
        "  location = datasets_location if datasets_location[-1] == '/' else f'{datasets_location}/'\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from: {location} ...')\n",
        "\n",
        "  assert files_locations != 0, 'No files found in the provided location'\n",
        "\n",
        "  datasets_map = {}\n",
        "  for f in files_locations:\n",
        "    print(f'Reading dataset: {f} ...')\n",
        "    dataset = pd.read_csv(f)\n",
        "    datasets_map[extract_file_name(f)] = dataset\n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "\n",
        "  return datasets_map\n",
        "\n",
        "\n",
        "def create_results_folder(loc):\n",
        "  parent_location = os.path.abspath(os.path.join(loc, os.pardir))\n",
        "  mod_location = parent_location if parent_location[-1] == '/' else f'{parent_location}/'\n",
        "\n",
        "  results_location = f'{mod_location}results'\n",
        "\n",
        "  if not os.path.exists(results_location):\n",
        "    os.makedirs(results_location)\n",
        "\n",
        "  print(f'Created results dataset on location: {results_location} ...')\n",
        "\n",
        "  return results_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OThrhyy6Ct00"
      },
      "outputs": [],
      "source": [
        "tokenizer = torch.load(tokenizer_loc)\n",
        "\n",
        "lexicon_datasets_map = extract_datasets_map(lexicons_folder_loc)\n",
        "\n",
        "eval_datasets_map = extract_datasets_map(eval_datasets_folder_loc)\n",
        "\n",
        "results_folder_loc = create_results_folder(lexicons_folder_loc)\n",
        "\n",
        "df, df_one_line = create_summary_results(lexicon_name, lexicon_datasets_map, eval_datasets_map, results_folder_loc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_one_line"
      ],
      "metadata": {
        "id": "DHFrd-6lw01M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1GXR8L1Br6y1bRTJHdyMG_w6JHAyu254k",
      "authorship_tag": "ABX9TyNQAQbRehHRzm5/Z72uaGPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}