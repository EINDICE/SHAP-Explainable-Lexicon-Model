{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User input"
      ],
      "metadata": {
        "id": "K6HIf0rycoYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This session should be connected to GPU"
      ],
      "metadata": {
        "id": "6Xb023AKLsfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRmThHDwv7bj"
      },
      "outputs": [],
      "source": [
        "# enter the location of the sentiment clasification model\n",
        "model_loc = '/content/drive/MyDrive/roberta_model'\n",
        "\n",
        "# enter the location of the tokenizer\n",
        "tokenizer_loc = '/content/drive/MyDrive/roberta_tokenizer'\n",
        "\n",
        "# enter the source dataset location\n",
        "source_dataset_loc = '/content/drive/MyDrive/nasdaq.csv'\n",
        "\n",
        "# enter the folder where all subsequent datasets will be saved\n",
        "lexicon_folder_loc = '/content/drive/MyDrive/nasdaq'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTH2rxnizvGF"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVdPC3rlpl1j"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_transformers"
      ],
      "metadata": {
        "id": "aYv0LgBIj1j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "9M9EBnH7j7so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shortuuid"
      ],
      "metadata": {
        "id": "IsnUdGQHj9KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm3JN7cX9iV6"
      },
      "source": [
        "# Word Extraction Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBrMUFmWjvwO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "import shap\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "import shortuuid\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "class WordExtractor:\n",
        "\n",
        "  def __init__(self, model, explainer, drive_loc):\n",
        "    self.model = model\n",
        "    self.explainer = explainer\n",
        "    \n",
        "    self.drive_loc = drive_loc if drive_loc[-1] == '/' else f'{drive_loc}/'\n",
        "    self.log_uuid = shortuuid.uuid()\n",
        "\n",
        "    self.negative_class = 0\n",
        "    self.positive_class = 1\n",
        "    self.score_col = 'score'\n",
        "\n",
        "    self.log_df_list = []\n",
        "    self.positive_words_df_list = []\n",
        "    self.negative_words_df_list = []\n",
        "\n",
        "    self.log_columns = ['uuid', 'sentence', 'positive_score', 'negative_score', 'label', 'values', \"base_values\", \"data\"]\n",
        "    self.pos_neg_columns = ['word', 'count', 'shap_values', 'sentence_uuid']\n",
        "\n",
        "    self.tz_utc = pytz.timezone('UTC')\n",
        "\n",
        "\n",
        "  def extracting_sentence_label(self, sentence):\n",
        "    # prediction\n",
        "    pred = self.model([sentence])[0]\n",
        "\n",
        "    # extraction poitive and negative score from prediction\n",
        "    neg_score = pred[self.negative_class][self.score_col]\n",
        "    pos_score = pred[self.positive_class][self.score_col]\n",
        "\n",
        "    # determining the label\n",
        "    label = self.negative_class if neg_score >= pos_score else self.positive_class\n",
        "\n",
        "    return sentence, label, neg_score, pos_score\n",
        "\n",
        "  def shap_explaining_results(self, sentence):\n",
        "    # shap explaining the prediction of the model\n",
        "    shap_values = self.explainer([sentence])\n",
        "\n",
        "    # extract data from the prediction\n",
        "    words = shap_values.data[0]\n",
        "    # extract shap values from the prediction\n",
        "    values = shap_values.values[0]\n",
        "    # extract shap base values from the prediction\n",
        "    base_value = shap_values.base_values[0]\n",
        "    return words, values, base_value\n",
        "\n",
        "\n",
        "  def log_item(self, uuid, sentence, pos_score, neg_score, label, words, values, base_value):\n",
        "    self.log_df_list.append([uuid, sentence, pos_score, neg_score, label, values, base_value, words])\n",
        "\n",
        "  def add_new_word(self, word, probability, dataset, uuid):\n",
        "    dataset.append([word, 1, str(probability), uuid])\n",
        "\n",
        "  def save_location(self, location, type_dataset, prefix):\n",
        "    if location == 'drive':\n",
        "      return f'{self.drive_loc}{self.log_uuid}--{prefix}{type_dataset}.csv'\n",
        "\n",
        "    return f'/content/{self.log_uuid}--{prefix}{type_dataset}.csv'\n",
        "\n",
        "  def create_df(self):\n",
        "    # create dataframes from the lists\n",
        "    log_df = pd.DataFrame(self.log_df_list, columns = self.log_columns)\n",
        "    positive_df = pd.DataFrame(self.positive_words_df_list, columns = self.pos_neg_columns)\n",
        "    negative_df = pd.DataFrame(self.negative_words_df_list, columns = self.pos_neg_columns)\n",
        "\n",
        "    return log_df, positive_df, negative_df\n",
        "\n",
        "  def save_datasets(self, prefix='tmp_', location = None):\n",
        "    log_df, positive_df, negative_df = self.create_df()\n",
        "\n",
        "    log_df.to_csv(self.save_location(location, 'log_dataset', prefix), index=False)\n",
        "    positive_df.to_csv(self.save_location(location, 'positive_words', prefix), index=False)\n",
        "    negative_df.to_csv(self.save_location(location, 'negative_words', prefix), index=False)\n",
        "\n",
        "    return log_df, positive_df, negative_df\n",
        "\n",
        "\n",
        "  def print_log(self, count):\n",
        "    # loging in utc time zone\n",
        "    print('Printing line number ' + str(count))\n",
        "    datetime_utc = datetime.now(self.tz_utc)\n",
        "    print(\"Current time:\", datetime_utc.strftime(\"%H:%M:%S\"))\n",
        "    self.save_datasets()\n",
        "\n",
        "  def classify_word(self, word, shap_value, label, uuid):\n",
        "    word = str.lower(word)\n",
        "    if label == self.negative_class:\n",
        "      # if the score that is from the predicted label (negative) is greater or equal to 0, then add the words to the negative dataset, else add the same value, but abs to the positive dataset\n",
        "      if shap_value[label] >= 0:\n",
        "        self.add_new_word(word, shap_value[label], self.negative_words_df_list, uuid)\n",
        "      else:\n",
        "        self.add_new_word(word, abs(shap_value[label]), self.positive_words_df_list, uuid)\n",
        "\n",
        "    else:\n",
        "      # if the score that is from the predicted label (positive) is greater or equal to 0, then add the words to the positive dataset, else add the same value, but abs to the negative dataset\n",
        "      if shap_value[label] >= 0:\n",
        "        self.add_new_word(word, shap_value[label], self.positive_words_df_list, uuid)\n",
        "      else:\n",
        "        self.add_new_word(word, abs(shap_value[label]), self.negative_words_df_list, uuid)\n",
        "\n",
        "  def execute_process(self, sentence):\n",
        "    # creating sentence uuids that are added to the words extracted from that sentence\n",
        "    # this way we can debug and identify the origin sentences of one word in the dataset\n",
        "    uuid = shortuuid.uuid()\n",
        "\n",
        "    # log the progress on each 100 sentences\n",
        "    # save the datasets on each 100 sentences\n",
        "    # saving partial datasets in case when the session on Google Colab breaks\n",
        "    count = len(self.log_df_list)\n",
        "    if count % 100 == 0:\n",
        "      self.print_log(count)\n",
        "      self.save_datasets(location = 'drive')\n",
        "\n",
        "    # extracting the label, probability for the negative label and probability for the positive label\n",
        "    sentence, label, neg_score, pos_score = self.extracting_sentence_label(sentence)\n",
        "\n",
        "    # extract words, shap values and base values from SHAP explainer\n",
        "    words, values, base_value = self.shap_explaining_results(sentence)\n",
        "\n",
        "    # log the progress\n",
        "    self.log_item(uuid, sentence, pos_score, neg_score, label, words, values, base_value)\n",
        "\n",
        "    # classify the extracted words\n",
        "    [self.classify_word(word, shap_value, label, uuid) for word, shap_value in zip(words, values)]\n",
        "\n",
        "\n",
        "  def run_extraction(self, df, text_column):\n",
        "    start_number_point = self.extract_start_point()\n",
        "    sentences_left = len(df) - start_number_point\n",
        "    if sentences_left == 0:\n",
        "      raise Exception('No more sentences left for processing. Please continue with the next steps')\n",
        "    else:\n",
        "      print(f'{sentences_left} sentences are left for processing')\n",
        "\n",
        "    df[start_number_point:][text_column].apply(lambda sentence: self.execute_process(sentence))\n",
        "\n",
        "    self.save_datasets(prefix = '', location = 'drive')\n",
        "    return self.save_datasets('')\n",
        "\n",
        "\n",
        "  def extract_rows_number_from_execution(self, exec, files_locations):\n",
        "    exec_files = [f for f in files_locations if exec in f]\n",
        "    if len(exec_files) > 3:\n",
        "      final_files = [f for f in exec_files if 'tmp' not in f]\n",
        "    else:\n",
        "      final_files = exec_files.copy(True)\n",
        "\n",
        "    log_file = [f for f in final_files if 'log' in f][0]\n",
        "    log_df = pd.read_csv(log_file)\n",
        "    return len(log_df)\n",
        "\n",
        "  def extract_start_point(self):\n",
        "    files_locations = [join(self.drive_loc, f) for f in listdir(self.drive_loc) if isfile(join(self.drive_loc, f))]\n",
        "    if len(files_locations) == 0:\n",
        "      return 0\n",
        "\n",
        "    executions = set([f.split('/')[-1].split(sep='--')[0] for f in files_locations])\n",
        "    start_number_point = np.array([self.extract_rows_number_from_execution(exec, files_locations) for exec in executions]).sum()\n",
        "\n",
        "    return start_number_point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNJfk-sC9qRT"
      },
      "source": [
        "# Explainer creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaeQMGOUAvrt"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "#loading model and tokenizer from drive\n",
        "model = torch.load(model_loc)\n",
        "tokenizer = torch.load(tokenizer_loc)\n",
        "\n",
        "# For using it with CPU\n",
        "# model = torch.load(model_loc, map_location=torch.device('cpu'))\n",
        "# tokenizer = torch.load(tokenizer_loc, map_location=torch.device('cpu'))\n",
        "\n",
        "# creating pipeline for sentiment analysis\n",
        "md = pipeline('sentiment-analysis', model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
        "\n",
        "# For using it with CPU\n",
        "# md = pipeline('sentiment-analysis', model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "# setting sigmoid as activation function \n",
        "md.function_to_apply = 'sigmoid'\n",
        "\n",
        "# creating shap transformer pipeline\n",
        "shap_p = shap.models.TransformersPipeline(md, rescale_to_logits=False)\n",
        "# creating shap explainer\n",
        "explainer = shap.Explainer(shap_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Words Collection"
      ],
      "metadata": {
        "id": "3Cr8nL_LdUE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the source dataset\n",
        "df = pd.read_csv(source_dataset_loc)\n",
        "\n",
        "# creating the word extractor object\n",
        "word_extractor = WordExtractor(md, explainer, lexicon_folder_loc)\n",
        "# extracting the words\n",
        "log_df, positive_df, negative_df = word_extractor.run_extraction(df, 'text')"
      ],
      "metadata": {
        "id": "7TrtjWq8dWPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
