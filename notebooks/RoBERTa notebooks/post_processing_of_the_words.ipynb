{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18FNUKaAxeteIj60uoc6VaAn8gH-dSoyE",
      "authorship_tag": "ABX9TyNabCjnqahCbG8ZbzFdfoJE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User input"
      ],
      "metadata": {
        "id": "CVqzNxESaLVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter folder drive location of the extracted (final) datasets\n",
        "drive_location = '/content/drive/MyDrive/nasdaq/concatenated datasets'\n",
        "\n",
        "# enter Loughran-McDonald (LM) positive words dictionary location\n",
        "lm_positive_words_location = '/content/drive/MyDrive/datasets/source datasets/lmd_positive_words.csv'\n",
        "\n",
        "# enter Loughran-McDonald (LM) negative words dictionary location\n",
        "lm_negative_words_location = '/content/drive/MyDrive/datasets/source datasets/lmd_negative_words.csv'"
      ],
      "metadata": {
        "id": "TDdfYzMyaNKJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Processor Class"
      ],
      "metadata": {
        "id": "wx3Je8wZQk5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install english-words==1.1.0"
      ],
      "metadata": {
        "id": "pv7-vXvhrRZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHF9gDiQtlim"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import nltk\n",
        "import gdown\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from english_words import english_words_lower_alpha_set as eng_words\n",
        "\n",
        "class PostProcessor:\n",
        "\n",
        "  def __init__(self, datasets, lm_datasets, word_column, count_column, shap_values_column, sentence_uuid_column = None):\n",
        "    self.datasets = [dataset.copy(True) for dataset in datasets]\n",
        "    self.lm_datasets = lm_datasets\n",
        "\n",
        "    self.word_column = word_column\n",
        "    self.count_column = count_column\n",
        "    self.shap_values_column = shap_values_column\n",
        "    self.sentence_uuid_column = sentence_uuid_column\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    self.max_shap_value_column = f'max_{self.shap_values_column}'\n",
        "    self.min_shap_value_column = f'min_{self.shap_values_column}'\n",
        "    self.average_shap_value_column = f'average_{self.shap_values_column}'\n",
        "    self.sum_shap_value_column = f'sum_{self.shap_values_column}'\n",
        "\n",
        "    self.total_count_column = 'total'\n",
        "\n",
        "  def __clean_words(self, dataset):\n",
        "    # cleaning words such that special character ġ from RoBERTa tokenizator is replaced, the words are striped and changed to lower cases\n",
        "    dataset[self.word_column] = dataset[self.word_column].apply(lambda word: str(word).replace('ġ', '').strip().lower())\n",
        "    # words with length < 3 are dropped\n",
        "    dataset.drop(dataset[dataset[self.word_column].str.len() < 3].index, inplace = True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __get_wordnet_pos(self, word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  def __lemmatize_words(self, dataset):\n",
        "    dataset[self.word_column] = dataset[self.word_column].apply(lambda word: self.lemmatizer.lemmatize(word, self.__get_wordnet_pos(word)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __remove_stop_words(self, dataset):\n",
        "    dataset = dataset[~dataset[self.word_column].isin(STOPWORDS)]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __create_additional_columns(self, dataset):\n",
        "    # adding max min and average shap value column\n",
        "    # initially they are all equal\n",
        "    dataset[self.max_shap_value_column] = dataset[self.shap_values_column]\n",
        "    dataset[self.min_shap_value_column] = dataset[self.shap_values_column]\n",
        "    dataset[self.average_shap_value_column] = dataset[self.shap_values_column]\n",
        "\n",
        "    # adding total count column which in this moment is equal to count column\n",
        "    dataset[self.total_count_column] = dataset[self.count_column]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  def __group_duplicate_words(self, dataset):\n",
        "    self.__create_additional_columns(dataset)\n",
        "\n",
        "    # group, then aggreagate words by function\n",
        "    dataset = dataset.groupby(by=[self.word_column]).agg(\n",
        "        {self.count_column: 'sum', self.total_count_column: 'sum', self.shap_values_column: 'sum', self.average_shap_value_column: 'mean', \n",
        "         self.max_shap_value_column: 'max', self.min_shap_value_column: 'min', self.sentence_uuid_column: ','.join}).reset_index()\n",
        "\n",
        "    # changing the name from shap_values_column to sum_shap_value_column\n",
        "    columns = list(dataset.columns)\n",
        "    index_to_change = columns.index(self.shap_values_column)\n",
        "    columns[index_to_change] = self.sum_shap_value_column\n",
        "\n",
        "    dataset.columns = columns\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def process_dataset(self, dataset):\n",
        "    modified_dataset = self.__clean_words(dataset)\n",
        "\n",
        "    modified_dataset = self.__group_duplicate_words(modified_dataset)\n",
        "    modified_dataset = self.__lemmatize_words(modified_dataset)\n",
        "    modified_dataset = self.__remove_stop_words(modified_dataset)\n",
        "\n",
        "    return modified_dataset\n",
        "\n",
        "  def clean_lm_words(self, dataset):\n",
        "    # strip and change to lower cases lm words\n",
        "    dataset[self.word_column] = dataset[self.word_column].apply(lambda word: str(word).strip().lower())\n",
        "    return self.__lemmatize_words(dataset)\n",
        "\n",
        "  def word_exist_in_dictionary(self, word):\n",
        "    # checking if the word exists in dictionary\n",
        "    return word in eng_words\n",
        "\n",
        "  def remove_non_dictionary_words_from_dataset(self, dataset):\n",
        "    # finding all words from the dataset that don't exist in a dictionary\n",
        "    throw_out_words = [word for word in dataset[self.word_column].unique() if not self.word_exist_in_dictionary(word)]\n",
        "\n",
        "    # removing all words that don't exist in a dictionary\n",
        "    dataset = dataset.loc[~dataset[self.word_column].isin(throw_out_words)]\n",
        "  \n",
        "    return dataset\n",
        "\n",
        "  def run_processing_on_datasets(self):\n",
        "    datasets = [self.process_dataset(dataset.copy(True)) for dataset in self.datasets]\n",
        "    lm_datasets = [self.clean_lm_words(lm_dataset.copy(True)) for lm_dataset in self.lm_datasets]\n",
        "\n",
        "    datasets = [self.remove_non_dictionary_words_from_dataset(dataset) for dataset in datasets]\n",
        "\n",
        "    return datasets, lm_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unnecessary_words(df):\n",
        "  df_copy = df.sort_values(by='word').copy(True)\n",
        "\n",
        "  not_words = [instance for instance in df_copy['word'].values if (not instance.isalpha()) or instance.isnumeric()]\n",
        "  print(f'Removed {len(not_words)} instances')\n",
        "  return df_copy[~df_copy['word'].isin(not_words)]"
      ],
      "metadata": {
        "id": "0ocfrmSuE7Rr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def get_word_datasets(location):\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from location: {location}')\n",
        "\n",
        "  assert len(files_locations) != 0, 'No files in the provided location'\n",
        "\n",
        "  pos_loc = [f for f in files_locations if 'positive' in f]\n",
        "  neg_loc = [f for f in files_locations if 'negative' in f]\n",
        "\n",
        "  if not pos_loc:\n",
        "    raise Exception('Positive words dataset was not found')\n",
        "  if not neg_loc:\n",
        "    raise Exception('Negative words dataset was not found')\n",
        "  \n",
        "  print(f'Reading {pos_loc}, {neg_loc} ...')\n",
        "\n",
        "  positive_words_df = pd.read_csv(pos_loc[0])\n",
        "  negative_words_df = pd.read_csv(neg_loc[0])\n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "  \n",
        "  return positive_words_df, negative_words_df\n",
        "\n",
        "def save_datasets(location, datasets_map):\n",
        "  location_mod = location if location[-1] == '/' else f'{location}/'\n",
        "  datasets_folder_loc = f'{location_mod}processed/'\n",
        "\n",
        "  if not os.path.exists(datasets_folder_loc):\n",
        "    os.makedirs(datasets_folder_loc)\n",
        "\n",
        "  for dataset_name in datasets_map:\n",
        "    file_name = f'{datasets_folder_loc}{dataset_name}'\n",
        "    datasets_map[dataset_name].to_csv(file_name, index = False)\n",
        "\n",
        "  print(f'Datasets saved on location: {datasets_folder_loc}')"
      ],
      "metadata": {
        "id": "pqJfkOJiaii3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post processing of the words"
      ],
      "metadata": {
        "id": "lcN3sKmJaXVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading positive and negative extracted words from drive\n",
        "positive_words, negative_words = get_word_datasets(drive_location)\n",
        "\n",
        "\n",
        "# read the positive and negative LM words\n",
        "lm_positive_words = pd.read_csv(lm_positive_words_location)\n",
        "lm_negative_words = pd.read_csv(lm_negative_words_location)"
      ],
      "metadata": {
        "id": "TaxBtHEvaXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating PostProcessor object with the shap generated sentiment datasets, the LM sentiment datasets, as well as the column that point to the words, their count, ther shap values, and uuid\n",
        "post_processor = PostProcessor([positive_words, negative_words], [lm_positive_words, lm_negative_words], 'word', 'count', 'shap_values', 'sentence_uuid')\n",
        "\n",
        "# running the processing of the words\n",
        "# output: the processed sentiment datasets for both the shap generated ones and LM\n",
        "our, lm = post_processor.run_processing_on_datasets()\n",
        "\n",
        "# extracting the processed sentiment datasets\n",
        "positive_words, negative_words = our\n",
        "lm_positive_words, lm_negative_words = lm\n",
        "\n",
        "\n",
        "# filtering the shap generated sentiment dataset in order to avoid the unnecassary \"words\" (the ones that are not actually words)\n",
        "positive_words = find_unnecessary_words(positive_words)\n",
        "negative_words = find_unnecessary_words(negative_words)\n",
        "\n",
        "\n",
        "datasets_map = {'processed-positive_words.csv': positive_words, 'processed-negative_words.csv': negative_words,\n",
        "                'processed-lm_positive_words.csv': lm_positive_words, 'processed-lm_negative_words.csv': lm_negative_words}\n",
        "\n",
        "# datasets are saved on Google Drive, folder named as 'processed'\n",
        "save_datasets(drive_location, datasets_map)"
      ],
      "metadata": {
        "id": "U4FVuMHRaXVn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
