{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1scalG4JSWL_Ri5JpCXl07wayD4n9aw7H",
      "authorship_tag": "ABX9TyMDM6XC9a1OheJcgzVG8HKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hristijanpeshov/SHAP-Explainable-Lexicon-Model/blob/master/FinBERT_concatenate_result_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User input"
      ],
      "metadata": {
        "id": "fNGCuo-7ArFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter the lexicon_folder_loc value from the \"initial_words_collecting.ipynb\" notebook (the folder where the extracted words' datasets are stored)\n",
        "drive_folder_location = '/content/drive/MyDrive/finbert process/nasdaq'\n",
        "\n",
        "# ented source dataset location\n",
        "dataset_file_location = '/content/drive/MyDrive/datasets/source datasets/nasdaq.csv'"
      ],
      "metadata": {
        "id": "X4fkmjZbAtVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "BP6TNYpjAqmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def get_execution_data_frame(files, file_type):\n",
        "  exec_file = [f for f in files if file_type in f][0]\n",
        "  exec_df = pd.read_csv(exec_file)\n",
        "\n",
        "  return exec_df\n",
        "\n",
        "def extract_dfs_from_execution(exec, files_locations):\n",
        "  exec_files = [f for f in files_locations if exec in f]\n",
        "  if len(exec_files) > 3:\n",
        "    final_files = [f for f in exec_files if 'tmp' not in f]\n",
        "  else:\n",
        "    final_files = exec_files.copy()\n",
        "\n",
        "  log_df = get_execution_data_frame(final_files, 'log')\n",
        "  positive_words_df = get_execution_data_frame(final_files, 'positive')\n",
        "  negative_words_df = get_execution_data_frame(final_files, 'negative')\n",
        "\n",
        "  return log_df, positive_words_df, negative_words_df\n",
        "\n",
        "def extract_final_dfs(all_executions):\n",
        "  all_log_dfs = all_executions[0][0].copy(True)\n",
        "  all_pos_dfs = all_executions[0][1].copy(True)\n",
        "  all_neg_dfs = all_executions[0][2].copy(True)\n",
        "\n",
        "  for exec in all_executions[1:]:\n",
        "    all_log_dfs = all_log_dfs.append(exec[0], ignore_index=True)\n",
        "    all_pos_dfs = all_pos_dfs.append(exec[1], ignore_index=True)\n",
        "    all_neg_dfs = all_neg_dfs.append(exec[2], ignore_index=True)\n",
        "\n",
        "  return all_log_dfs, all_pos_dfs, all_neg_dfs\n",
        "\n",
        "def save_df(df, loc):\n",
        "  df.to_csv(loc, index=False)\n",
        "\n",
        "def concatenate_datasets(drive_loc, dataset_loc):\n",
        "  files_locations = [join(drive_loc, f) for f in listdir(drive_loc) if isfile(join(drive_loc, f))]\n",
        "\n",
        "  assert len(files_locations) != 0, f'No files found in the provided location: [{drive_loc}]'\n",
        "\n",
        "  executions = set([f.split('/')[-1].split(sep='--')[0] for f in files_locations])\n",
        "  all_executions_dfs = [extract_dfs_from_execution(exec, files_locations) for exec in executions]\n",
        "\n",
        "  final_log_df, final_pos_df, final_neg_df = extract_final_dfs(all_executions_dfs)\n",
        "\n",
        "  dataset = pd.read_csv(dataset_loc)\n",
        "\n",
        "  assert len(dataset) == len(final_log_df), 'Words extraction was not executed completely. Please complete the extraction of words from the dataset'\n",
        "\n",
        "  drive_loc_mod = drive_loc if drive_loc[-1] == '/' else f'{drive_loc}/'\n",
        "  conc_drive_loc = f'{drive_loc_mod}concatenated datasets'\n",
        "  if not os.path.exists(conc_drive_loc):\n",
        "    os.makedirs(conc_drive_loc)\n",
        "\n",
        "  save_df(final_log_df, f'{conc_drive_loc}/log_dataset.csv')\n",
        "  save_df(final_pos_df, f'{conc_drive_loc}/positive_words.csv')\n",
        "  save_df(final_neg_df, f'{conc_drive_loc}/negative_words.csv')\n",
        "\n",
        "  print(f'Datasets saved to folder: {conc_drive_loc}')\n",
        "\n",
        "  return final_log_df, final_pos_df, final_neg_df"
      ],
      "metadata": {
        "id": "HP4ZYFdEFIN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concatenate datasets"
      ],
      "metadata": {
        "id": "ZEPtDmHWPXfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs, positive_words, negative_words = concatenate_datasets(drive_folder_location, dataset_file_location)"
      ],
      "metadata": {
        "id": "ZW8Ow7NMPXFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}