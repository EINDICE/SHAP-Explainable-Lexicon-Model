{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hristijanpeshov/SHAP-Explainable-Lexicon-Model/blob/master/notebooks/FinBERT%20notebooks/FinBERT_coefficients_grid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMjZzmB6PTm"
      },
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twAm5QQn6PJG"
      },
      "outputs": [],
      "source": [
        "# enter the location of lexicons (please make sure that there are only lexicons files in the folder)\n",
        "lexicons_folder_map = {\n",
        "    'nasdaq': '/content/drive/MyDrive/finbert process/nasdaq/concatenated datasets/lexicons',\n",
        "    'fpb': '/content/drive/MyDrive/finbert process/fpb/concatenated datasets/lexicons',\n",
        "    'sentfin': '/content/drive/MyDrive/finbert process/sentfin/concatenated datasets/lexicons'\n",
        "}\n",
        "\n",
        "# enter the location of all evaluation datasets (please make sure that there are only evaluation files in the folder)\n",
        "eval_datasets_folder_loc = '/content/drive/MyDrive/datasets/evaluation datasets'\n",
        "\n",
        "# enter the location where the best performance coefficient results should be saved\n",
        "best_coef_results_loc = '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8tFPSCnYtEv"
      },
      "source": [
        "# ShapDictModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOLcHcCZL6F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqVEyjNMQvwG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import nltk\n",
        "import torch\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ShapDictModel:\n",
        "  def __init__(self, dataset, tokenizer, word_column, category_column, decision_makers, count_column, dataset_source = None):\n",
        "    self.dataset_source = str.upper(dataset_source) if dataset_source is not None else 'both'\n",
        "    self.word_column = word_column\n",
        "    self.tokenizer = tokenizer\n",
        "    self.count_column = count_column\n",
        "\n",
        "    # Lemmatizer\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # source column values\n",
        "    self.lm_source = 'LM'\n",
        "    self.tm_source = 'OUR_WORDS'\n",
        "\n",
        "    # column prefix\n",
        "    self.lm_prefix = 'LM_'\n",
        "    self.tm_prefix = 'TM_'\n",
        "\n",
        "    # category values\n",
        "    self.positive_category_value = 'positive'\n",
        "    self.negative_category_value = 'negative'\n",
        "\n",
        "    # opposite prefix\n",
        "    self.opposite_prefix = 'opposite_'\n",
        "\n",
        "    # number of required coefficient when both sources or one sorce\n",
        "    self.coefficient_number_both_sources = 4\n",
        "    self.coefficient_number_one_source = 2\n",
        "\n",
        "    # prefix when one source is chosen\n",
        "    self.prefix = 'TM_' if self.dataset_source == self.tm_source else 'LM_' if self.dataset_source == self.lm_source else ''\n",
        "\n",
        "    # source column postfix\n",
        "    self.source_column = 'src'\n",
        "\n",
        "    # dataset on which results are calculated\n",
        "    self.dataset = dataset if self.dataset_source == 'both' else self.extract_dataset_from_source(dataset, self.dataset_source)\n",
        "\n",
        "    # category when one source is chosen\n",
        "    self.category = category_column\n",
        "\n",
        "    # decision makers\n",
        "    self.decision_makers = decision_makers\n",
        "    print()\n",
        "    print(f'Created ShapDictModel with decision makers: {self.decision_makers}')\n",
        "    print()\n",
        "\n",
        "\n",
        "  def extract_dataset_from_source(self, dataset, source, only_source_columns=True):\n",
        "    # depending on which source is chosen, the full dataset will be modified to return the required dataset\n",
        "\n",
        "    # if LM is chosen as source, then the returned dataset will contain only the words that were originally from the LM dataset with LM_ added as prefix to the columns\n",
        "    if source == self.lm_source:\n",
        "      prefix = self.lm_prefix\n",
        "      column = f'{self.lm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.tm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.lm_source\n",
        "      opposite_dataset_source = self.tm_source\n",
        "    else:\n",
        "      # if OUR_WORDS is chosen as source, then the returned dataset will contain only the words that were originally from our words dataset with TM_ added as prefix to the columns\n",
        "      prefix = self.tm_prefix\n",
        "      column = f'{self.tm_prefix}{self.source_column}'\n",
        "      opposite_column = f'{self.lm_prefix}{self.source_column}'\n",
        "\n",
        "      dataset_source = self.tm_source\n",
        "      opposite_dataset_source = self.lm_source\n",
        "\n",
        "    source_dataset = dataset[(dataset[column] == dataset_source) & ((dataset[opposite_column] == dataset_source) | (dataset[opposite_column] == opposite_dataset_source))]\n",
        "\n",
        "    # filtering so just the necessary columns will remain\n",
        "    if only_source_columns:\n",
        "      columns = list(source_dataset.columns)\n",
        "      source_columns = [self.word_column] + [column for column in columns if prefix in column]\n",
        "\n",
        "      return source_dataset[source_columns]\n",
        "\n",
        "    return source_dataset\n",
        "\n",
        "  def calculate_score_both_dataset_sources(self, word_occurence):\n",
        "    tm_accumulated_score, tm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.tm_prefix)\n",
        "    lm_accumulated_score, lm_opposite_accumulated_score = self.calculate_dataset_source_score(word_occurence, self.lm_prefix)\n",
        "\n",
        "    return tm_accumulated_score, tm_opposite_accumulated_score, lm_accumulated_score, lm_opposite_accumulated_score\n",
        "\n",
        "  def calculate_score_one_dataset_source(self, words, prefix):\n",
        "    accumulated_score, opposite_accumulated_score = 0, 0\n",
        "\n",
        "    for word in words:\n",
        "      # it can only have one occurence, but to check if it occurs at all\n",
        "      word_occurences = self.dataset.loc[self.dataset[self.word_column] == word].values\n",
        "      if len(word_occurences) == 0:\n",
        "        continue\n",
        "\n",
        "      word_occurence = word_occurences[0]\n",
        "\n",
        "      decision_scores = self.calculate_dataset_source_score(word_occurence, prefix)\n",
        "      accumulated_score += decision_scores[0]\n",
        "      opposite_accumulated_score += decision_scores[1]\n",
        "\n",
        "    return accumulated_score, opposite_accumulated_score\n",
        "\n",
        "  def predict_sentence_label(self, sentence, label_t):\n",
        "    tm_words = self.clean_and_lemmatize_words(self.tokenizer.tokenize(sentence))\n",
        "    lm_words = self.clean_and_lemmatize_words(word_tokenize(sentence))\n",
        "\n",
        "    tm, tmo = self.calculate_score_one_dataset_source(tm_words, self.tm_prefix)\n",
        "\n",
        "    lm, lmo = self.calculate_score_one_dataset_source(lm_words, self.lm_prefix)\n",
        "\n",
        "    return tm, tmo, lm, lmo\n",
        "\n",
        "  def __get_wordnet_pos(self, word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  def clean_and_lemmatize_words(self, words):\n",
        "    lower_case_words = [str(word).lower().replace('Ä¡', '').strip() for word in words]\n",
        "    return [self.lemmatizer.lemmatize(word, self.__get_wordnet_pos(word)) for word in lower_case_words]\n",
        "\n",
        "  def calculate_word_dm_score(self, word_occurence, decision_maker_column, count_column):\n",
        "    columns = list(self.dataset.columns)\n",
        "\n",
        "    decision_maker_index = columns.index(decision_maker_column)\n",
        "\n",
        "    value = word_occurence[decision_maker_index]\n",
        "\n",
        "    return value\n",
        "\n",
        "  def calculate_dataset_source_score(self, word_occurence, column_prefix):\n",
        "    columns = list(self.dataset.columns)\n",
        "    category_index = columns.index(f'{column_prefix}{self.category}')\n",
        "\n",
        "    word_category = word_occurence[category_index]\n",
        "\n",
        "    selected_category_sign = 1\n",
        "    opposite_category_sign = 1\n",
        "    if word_category == self.positive_category_value:\n",
        "      opposite_category_sign = -1\n",
        "    elif word_category == self.negative_category_value:\n",
        "      selected_category_sign = -1\n",
        "\n",
        "    if selected_category_sign == opposite_category_sign:\n",
        "      return 0, 0\n",
        "\n",
        "    opposite_column_prefix = f'{column_prefix}{self.opposite_prefix}'\n",
        "\n",
        "    selected_category_score = 0\n",
        "    opposite_category_score = 0\n",
        "    for decision_maker in self.decision_makers:\n",
        "      selected_category_score += self.calculate_word_dm_score(word_occurence, f'{column_prefix}{decision_maker}', f'{column_prefix}{self.count_column}') * selected_category_sign\n",
        "      opposite_category_score += self.calculate_word_dm_score(word_occurence, f'{opposite_column_prefix}{decision_maker}', f'{opposite_column_prefix}{self.count_column}') * opposite_category_sign\n",
        "\n",
        "    return selected_category_score, opposite_category_score\n",
        "\n",
        "  def calculate_model_accuracy(self, true_labels, predicted_labels):\n",
        "    accuracy_indicators = [true_label == predicted_label for true_label, predicted_label in zip(true_labels, predicted_labels)]\n",
        "\n",
        "    return np.asarray(accuracy_indicators).sum() / len(true_labels)\n",
        "\n",
        "\n",
        "  def predict_and_evaluate(self, sentences, true_labels):\n",
        "\n",
        "    calculated_scores = [self.predict_sentence_label(sentence, label) for sentence, label in zip(sentences, true_labels)]\n",
        "\n",
        "    return calculated_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS3kUmiCdvym"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pytz\n",
        "\n",
        "def create_summary_results(lexicon_source, lexicon_datasets, evaluation_datasets, drive_loc):\n",
        "\n",
        "  summary_df_items = []\n",
        "  for lexicon_name in lexicon_datasets:\n",
        "    lexicon = lexicon_datasets[lexicon_name]\n",
        "    lexicon_normalized = 'normalized' in lexicon_name\n",
        "\n",
        "    for evaluate_dataset_name in evaluation_datasets:\n",
        "      evaluate_dataset = evaluation_datasets[evaluate_dataset_name]\n",
        "      sentences = evaluate_dataset['text'].values\n",
        "      true_labels = evaluate_dataset['sentiment'].values\n",
        "\n",
        "      dataset = lexicon.copy(True)\n",
        "\n",
        "      lexicon_type = 'normalized' if lexicon_normalized else 'merged'\n",
        "\n",
        "      evaluation_result = evaluate(dataset, sentences, true_labels, lexicon_source, lexicon_normalized, evaluate_dataset_name)\n",
        "      summary_df_items = summary_df_items + evaluation_result\n",
        "\n",
        "  summary_df = pd.DataFrame(summary_df_items, columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset',\n",
        "                                                         'Sentence', 'True Label', 'XLex AS', 'XLex OAS', 'LM AS', 'LM OAS'])\n",
        "\n",
        "  summary_df.to_csv(f'{drive_loc}/summary_df.csv', index=False)\n",
        "\n",
        "  return summary_df\n",
        "\n",
        "def run_shap_dict_model(dataset, sentences, true_labels, dataset_source = None):\n",
        "  shap_dict_model = ShapDictModel(dataset, tokenizer, 'word', 'category', ['average_shap_values'], 'count', dataset_source = dataset_source)\n",
        "  result = shap_dict_model.predict_and_evaluate(sentences, true_labels)\n",
        "\n",
        "  return result\n",
        "\n",
        "def evaluate(dataset, sentences, true_labels, lexicon_source, lexicon_name, evaluate_dataset_name):\n",
        "  results = run_shap_dict_model(dataset, sentences, true_labels)\n",
        "\n",
        "  res_df_list = []\n",
        "  for sentence, label, res in zip(sentences, true_labels, results):\n",
        "    new_entry = [lexicon_source, lexicon_name, evaluate_dataset_name, sentence, label] + list(res)\n",
        "    res_df_list.append(new_entry)\n",
        "\n",
        "  return res_df_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fYDNTncaX_r"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "def calc_metrics(true_labels, predicted_labels):\n",
        "  results = []\n",
        "\n",
        "  acc = accuracy_score(true_labels, predicted_labels)\n",
        "  results.append(acc)\n",
        "\n",
        "  pr = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  results.append(pr)\n",
        "\n",
        "  rec = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  results.append(rec)\n",
        "\n",
        "  f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "  results.append(f1)\n",
        "\n",
        "  mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
        "  results.append(mcc)\n",
        "\n",
        "  cl_report = classification_report(true_labels, predicted_labels, zero_division=0)\n",
        "\n",
        "  conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def calc_label(decision_values):\n",
        "  return [1 if dv > 0 else 0 if dv < 0 else -1 for dv in decision_values]\n",
        "\n",
        "def calc_segment(coefs, df):\n",
        "  c1, c2, c3, c4 = coefs\n",
        "\n",
        "  xlex_decision_score = c1 * df['XLex AS'] + c2 * df['XLex OAS']\n",
        "  lm_decision_score =  c3 * df['LM AS'] + c4 * df['LM OAS']\n",
        "\n",
        "  return calc_label(lm_decision_score), calc_label(xlex_decision_score), calc_label(xlex_decision_score + lm_decision_score)\n",
        "\n",
        "\n",
        "def calc_row(coefs, dff, version, eval_df, extension = ''):\n",
        "  types = ['LMD', 'OUR', 'OUR + LMD']\n",
        "  true_labels = dff['True Label']\n",
        "  lex_name = dff['Lexicon Source'].values[0]\n",
        "  coef_df_values = []\n",
        "\n",
        "  whole_dataset = calc_segment(coefs, dff)\n",
        "  for wd, t in zip(whole_dataset, types):\n",
        "    metrics = calc_metrics(true_labels, wd)\n",
        "    new_row = [lex_name, version, eval_df, f'{t}{extension}', 'average_shap_values'] + coefs + metrics\n",
        "    coef_df_values.append(new_row)\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def calc_version(df, coefs, eval_dfs, version):\n",
        "  coef_df_values = []\n",
        "\n",
        "  for eval_df in eval_dfs:\n",
        "    dff = df[(df['Lexicon Normalized'] == version) & (df['Evaluation Dataset'] == eval_df)].copy(True).reset_index(drop=True)\n",
        "    coef_df_values = coef_df_values + calc_row(coefs, dff, version, eval_df)\n",
        "\n",
        "    dff_on_lm = dff[(dff['LM AS'] != 0) | (dff['LM OAS'] != 0)]\n",
        "    coef_df_values = coef_df_values + calc_row(coefs, dff_on_lm, version, eval_df, extension = ' on LMD')\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def calc_df(coefs, df):\n",
        "  versions = df['Lexicon Normalized'].unique()\n",
        "  eval_dfs = df['Evaluation Dataset'].unique()\n",
        "\n",
        "  coef_df_values = []\n",
        "\n",
        "  for version in versions:\n",
        "    coef_df_values = coef_df_values + calc_version(df, coefs, eval_dfs, version)\n",
        "\n",
        "  return coef_df_values\n",
        "\n",
        "def get_coefs_res(df, coefs):\n",
        "  columns = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Words Source',\n",
        "             'Decision Maker', 'C1', 'C2', 'C3', 'C4', 'Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
        "\n",
        "  dff = df.copy(True)\n",
        "\n",
        "  coefs_map = {}\n",
        "\n",
        "  for coef in coefs:\n",
        "    coef_df_values = calc_df(coef, dff)\n",
        "    coefs_map[str(coef)] = pd.DataFrame(coef_df_values, columns = columns)\n",
        "\n",
        "  return coefs_map\n",
        "\n",
        "\n",
        "def get_metric_values(df, eval_df, normalized, metric):\n",
        "  word_sources = ['LMD', 'OUR', 'OUR + LMD', 'LMD on LMD', 'OUR on LMD', 'OUR + LMD on LMD']\n",
        "  all_metric_values = []\n",
        "\n",
        "  for ws in word_sources:\n",
        "    eval_df_mask = df['Evaluation Dataset'] == eval_df\n",
        "    lexicon_normalized_mask = df['Lexicon Normalized'] == normalized\n",
        "    word_source_mask = df['Words Source'] == ws\n",
        "\n",
        "    combined_mask = eval_df_mask & lexicon_normalized_mask & word_source_mask\n",
        "\n",
        "    metric_value = df[combined_mask][metric].values[0]\n",
        "\n",
        "    all_metric_values.append(metric_value)\n",
        "\n",
        "  return all_metric_values\n",
        "\n",
        "def create_summary_dataset(df, metric):\n",
        "  source_df = df['Lexicon Source'].unique()[0]\n",
        "  eval_dfs = df['Evaluation Dataset'].unique()\n",
        "  normalized = True\n",
        "  coefs = list(df.loc[0, ['C1', 'C2', 'C3', 'C4']])\n",
        "  decision_maker = 'average_shap_values'\n",
        "\n",
        "  summary_df_values = []\n",
        "\n",
        "  for ed in eval_dfs:\n",
        "\n",
        "    for n in [normalized, not normalized]:\n",
        "      metric_values = get_metric_values(df, ed, n, metric)\n",
        "      row_value = [source_df, n, ed, decision_maker] + coefs + metric_values\n",
        "      summary_df_values.append(row_value)\n",
        "\n",
        "  cols = ['Lexicon Source', 'Lexicon Normalized', 'Evaluation Dataset', 'Decision Maker', 'C1', 'C2', 'C3', 'C4',\n",
        "          'LM', 'XLex', 'XLex + LM', 'LM on LM', 'XLex on LM', 'XLex + LM on LM']\n",
        "\n",
        "  return pd.DataFrame(summary_df_values, columns = cols)\n",
        "\n",
        "\n",
        "def coefficient_permutations(coefs, coefs_number):\n",
        "  return [p for p in itertools.product(coefs, repeat=coefs_number)]\n",
        "\n",
        "def calc_metric_acc_ratio(dff):\n",
        "  cols = ['XLex', 'XLex + LM', 'XLex on LM', 'XLex + LM on LM']\n",
        "  cols_lm = ['LM', 'LM', 'LM on LM', 'LM on LM']\n",
        "\n",
        "  res = []\n",
        "  sum_res = []\n",
        "\n",
        "  for c, c_lm in zip(cols, cols_lm):\n",
        "    res.append((dff[c] >= dff[c_lm]).sum())\n",
        "\n",
        "\n",
        "    sum_res.append(dff[c].sum())\n",
        "    sum_res.append((dff[c] - dff[c_lm]).sum())\n",
        "\n",
        "    sum_res.append(dff[c].mean())\n",
        "    sum_res.append((dff[c] - dff[c_lm]).mean())\n",
        "\n",
        "  return res, sum_res\n",
        "\n",
        "def make_str(nums, total):\n",
        "  res = []\n",
        "\n",
        "  for num in nums:\n",
        "    res.append(f'{str(num)}/{str(total)}')\n",
        "\n",
        "  return res\n",
        "\n",
        "def calc_perc(nums, total):\n",
        "  res = []\n",
        "\n",
        "  for num in nums:\n",
        "    perc = (num / total) * 100\n",
        "    res.append(perc)\n",
        "\n",
        "  return res\n",
        "\n",
        "def calc_metrics_ratio(dff):\n",
        "  metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
        "  rows = []\n",
        "\n",
        "  for metric in metrics:\n",
        "    df_metrics = pd.DataFrame()\n",
        "\n",
        "    summary_dataset = create_summary_dataset(dff, metric)\n",
        "    df_metrics = pd.concat([df_metrics, summary_dataset], ignore_index = True)\n",
        "\n",
        "    acc_ratios, sum_acc_ratios = calc_metric_acc_ratio(df_metrics)\n",
        "    total = len(df_metrics)\n",
        "\n",
        "    init_cols = df_metrics.loc[0, ['Lexicon Source', 'C1', 'C2', 'C3', 'C4']]\n",
        "\n",
        "    new_row = [init_cols[0]] + [metric] + list(init_cols[1:]) + make_str(acc_ratios, total) + calc_perc(acc_ratios, total) + sum_acc_ratios\n",
        "    rows.append(new_row)\n",
        "\n",
        "  return rows\n",
        "\n",
        "def calc_accuracy_ratio(df, coeffs):\n",
        "  cols = ['Lexicon Source', 'Metric', 'C1', 'C2', 'C3', 'C4', 'XLex', 'XLex + LM', 'XLex on LM', 'XLex + LM on LM',\n",
        "          '% XLex', '% XLex + LM', '% XLex on LM', '% XLex + LM on LM',\n",
        "          'XLex Sum Abs Value', 'XLex Sum Diff Value', 'XLex Avg Abs Value', 'XLex Avg Diff Value',\n",
        "          'XLex + LM Sum Abs Value', 'XLex + LM Sum Diff Value', 'XLex + LM Avg Abs Value', 'XLex + LM Avg Diff Value',\n",
        "          'XLex on LM Sum Abs Value', 'XLex on LM Sum Diff Value', 'XLex on LM Avg Abs Value', 'XLex on LM Avg Diff Value',\n",
        "          'XLex + LM on LM Sum Abs Value', 'XLex + LM on LM Sum Diff Value', 'XLex + LM on LM Avg Abs Value', 'XLex + LM on LM Avg Diff Value']\n",
        "  coefs_map = get_coefs_res(df, coeffs)\n",
        "  all_coef_dfs = list(coefs_map.values())\n",
        "\n",
        "  rows = []\n",
        "\n",
        "  for coef_df in all_coef_dfs:\n",
        "    new_rows = calc_metrics_ratio(coef_df)\n",
        "\n",
        "    rows = rows + new_rows\n",
        "\n",
        "  return pd.DataFrame(rows, columns = cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ZfUoZdNGPV"
      },
      "source": [
        "# Create Raw Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnm_ohd6wAy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def extract_file_name(file_loc):\n",
        "  return file_loc.split('/')[-1].split('.')[0]\n",
        "\n",
        "def extract_datasets_map(datasets_location):\n",
        "  location = datasets_location if datasets_location[-1] == '/' else f'{datasets_location}/'\n",
        "  files_locations = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n",
        "\n",
        "  print(f'Reading datasets from: {location} ...')\n",
        "\n",
        "  assert files_locations != 0, 'No files found in the provided location'\n",
        "\n",
        "  datasets_map = {}\n",
        "  for f in files_locations:\n",
        "    print(f'Reading dataset: {f} ...')\n",
        "    dataset = pd.read_csv(f)\n",
        "    datasets_map[extract_file_name(f)] = dataset\n",
        "\n",
        "  print(f'Reading datasets successfully finished ...')\n",
        "\n",
        "  return datasets_map\n",
        "\n",
        "\n",
        "def create_results_folder(loc):\n",
        "  parent_location = os.path.abspath(os.path.join(loc, os.pardir))\n",
        "  mod_location = parent_location if parent_location[-1] == '/' else f'{parent_location}/'\n",
        "\n",
        "  results_location = f'{mod_location}raw results'\n",
        "\n",
        "  if not os.path.exists(results_location):\n",
        "    os.makedirs(results_location)\n",
        "\n",
        "  print(f'Created results dataset on location: {results_location} ...')\n",
        "\n",
        "  return results_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OThrhyy6Ct00"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load FinBERT-Tone tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "raw_results_folder_map = {}\n",
        "\n",
        "for lexicon_name in lexicons_folder_map:\n",
        "  lexicon_folder_loc = lexicons_folder_map[lexicon_name]\n",
        "\n",
        "  lexicon_datasets_map = extract_datasets_map(lexicon_folder_loc)\n",
        "\n",
        "  eval_datasets_map = extract_datasets_map(eval_datasets_folder_loc)\n",
        "\n",
        "  results_folder_loc = create_results_folder(lexicon_folder_loc)\n",
        "\n",
        "  raw_results_folder_map[lexicon_name] = results_folder_loc\n",
        "\n",
        "  if lexicon_name == 'fpb':\n",
        "    filtered_dict = dict(filter(lambda item: item[0] != 'financial_phrase_bank', eval_datasets_map.items()))\n",
        "  else:\n",
        "    filtered_dict = eval_datasets_map\n",
        "\n",
        "  # creating raw results which will later be used to find the most suitable coefficients\n",
        "  df = create_summary_results(lexicon_name, lexicon_datasets_map, filtered_dict, results_folder_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbP0Tnqb-bTy"
      },
      "source": [
        "# Metric grid search from raw results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXiInMVMCM85"
      },
      "outputs": [],
      "source": [
        "summary_raw_results_map = {}\n",
        "\n",
        "for lex_name in raw_results_folder_map:\n",
        "  raw_result_folder = raw_results_folder_map[lex_name]\n",
        "  summary_raw_results_map[lex_name] = f'{raw_result_folder}/summary_df.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7z9KbqY_C11"
      },
      "outputs": [],
      "source": [
        "coefs = coefficient_permutations([0.1, 0.3, 0.5, 0.7, 0.9], 3)\n",
        "full_coefs = []\n",
        "for c in coefs:\n",
        "  full_coefs.append(list(c) + [0.5])\n",
        "\n",
        "\n",
        "metrics_grid_search_loc_map = {}\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "for lexicon_source in summary_raw_results_map:\n",
        "  summary_raw_results_loc = summary_raw_results_map[lexicon_source]\n",
        "\n",
        "  dff = pd.read_csv(summary_raw_results_loc)\n",
        "\n",
        "  full_dff = calc_accuracy_ratio(dff, full_coefs)\n",
        "\n",
        "  merged_df = pd.concat([merged_df, full_dff], ignore_index = True)\n",
        "\n",
        "  raw_result_folder_loc = raw_results_folder_map[lexicon_source]\n",
        "  parent_location = os.path.abspath(os.path.join(raw_result_folder_loc, os.pardir))\n",
        "  mod_location = parent_location if parent_location[-1] == '/' else f'{parent_location}/'\n",
        "\n",
        "  results_location = f'{mod_location}metrics_grid_search.csv'\n",
        "  full_dff.to_csv(results_location, index=False)\n",
        "\n",
        "  metrics_grid_search_loc_map[lexicon_source] = results_location\n",
        "\n",
        "best_coef_results_final_loc = best_coef_results_loc if best_coef_results_loc[-1] == '/' else f'{best_coef_results_loc}/'\n",
        "merged_df.to_csv(f'{best_coef_results_final_loc}best_coefs_all_dfs_summary.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvjIH8jDAOn"
      },
      "source": [
        "# Finding the best performance coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs0LLyjyDZ4R"
      },
      "outputs": [],
      "source": [
        "def calc_xlex_lm_combs(dataset, metrics, agg_comb):\n",
        "  l = []\n",
        "\n",
        "  for comb in xlex_lm_combinations:\n",
        "    s = 0\n",
        "    for c in comb:\n",
        "      s += calc_for_metrics(dataset, metrics, f'{c} {agg_comb} {value_end}')\n",
        "\n",
        "    l.append(s)\n",
        "\n",
        "  return l\n",
        "\n",
        "def calc_for_metrics(dataset, metrics, comb):\n",
        "  s = 0\n",
        "\n",
        "  for metric in metrics:\n",
        "    for source in lexicon_sources:\n",
        "      s += dataset[(dataset['Lexicon Source'] == source) & (dataset['Metric'] == metric)][comb].values[0]\n",
        "\n",
        "  return s\n",
        "\n",
        "def calc_best_options(dataset):\n",
        "  list_values = []\n",
        "\n",
        "  for coef in coefs:\n",
        "    coef_str = ','.join(str(value) for value in coef)\n",
        "    coef_dataset = dataset[(dataset['C1'] == coef[0]) & (dataset['C2'] == coef[1]) & (dataset['C3'] == coef[2]) & (dataset['C4'] == coef[3])].copy(True)\n",
        "\n",
        "    for agg_comb in agg_combinations:\n",
        "      for metrics, metric_type in zip([accuracy_metrics, primary_metrics, all_metrics], ['Accuracy', 'Primary', 'All']):\n",
        "        xlex_lm_combs_res = calc_xlex_lm_combs(coef_dataset, metrics, agg_comb)\n",
        "        new_row = [coef_str, agg_comb, metric_type] + xlex_lm_combs_res\n",
        "        list_values.append(new_row)\n",
        "\n",
        "  columns = ['Coefs', 'Agg Comb', 'Metric Type', 'Xlex, XLex + LM', 'XLex + LM,XLex + LM on LM', 'Xlex,XLex + LM,XLex + LM on LM', 'Xlex,XLex + LM,Xlex on LM,XLex + LM on LM']\n",
        "\n",
        "  return pd.DataFrame(list_values, columns = columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKAi8zeRjGLU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "best_coef_results_final_loc = best_coef_results_loc if best_coef_results_loc[-1] == '/' else f'{best_coef_results_loc}/'\n",
        "all_coefs_summary = pd.read_csv(f'{best_coef_results_final_loc}best_coefs_all_dfs_summary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZxLbrWHj9Nk"
      },
      "outputs": [],
      "source": [
        "coefs = full_coefs\n",
        "\n",
        "accuracy_metrics = ['Accuracy']\n",
        "primary_metrics = ['Accuracy', 'F1', 'MCC']\n",
        "all_metrics = primary_metrics + ['Precision', 'Recall']\n",
        "\n",
        "agg_combinations = ['Sum Abs', 'Sum Diff', 'Avg Abs', 'Avg Diff']\n",
        "value_end = 'Value'\n",
        "xlex_lm_combinations = [['XLex + LM', 'XLex'], ['XLex + LM', 'XLex + LM on LM'], ['XLex + LM', 'XLex + LM on LM', 'XLex'], ['XLex + LM', 'XLex + LM on LM', 'XLex', 'XLex on LM']]\n",
        "lexicon_sources = ['nasdaq', 'fpb', 'sentfin']\n",
        "\n",
        "\n",
        "new_df = calc_best_options(all_coefs_summary)\n",
        "new_df.to_csv(f'{best_coef_results_final_loc}final_coef_metric_evaluation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl2luZoTnWQ2"
      },
      "outputs": [],
      "source": [
        "metric_types = ['Primary']\n",
        "agg_types = ['Avg Abs']\n",
        "s_values = [['Xlex, XLex + LM']]\n",
        "\n",
        "coef_map = {}\n",
        "\n",
        "for s in s_values:\n",
        "  for agg in agg_types:\n",
        "    for metr in metric_types:\n",
        "      comb = new_df[(new_df['Metric Type'] == metr) & (new_df['Agg Comb'] == agg)].sort_values(by = s, ascending=[False] * len(s)).head(1).values[0][0]\n",
        "      if comb in coef_map:\n",
        "        coef_map[comb] = coef_map[comb] + 1\n",
        "      else:\n",
        "        coef_map[comb] = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coef_map"
      ],
      "metadata": {
        "id": "tFymObcdumAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1zjj2cBfnMO2P6Zbbhsox0fyNCo9F4_9w",
      "authorship_tag": "ABX9TyMPMbw/hguzQKpkjheE15Af",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}